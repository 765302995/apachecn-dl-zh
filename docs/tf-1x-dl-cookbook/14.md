# TensorFlow 处理单元

Google 服务（例如 Google 搜索（RankBrain），街景，Google 照片和 Google 翻译）有一个共同点：它们都使用 Google 的 Tensor 处理单元或 **TPU** 进行计算。

您可能在想什么是 TPU，这些服务有什么好处？ 所有这些服务都在后台使用最新的机器学习算法，并且这些算法涉及大量计算。 TPU 有助于加速所涉及的神经网络计算。 甚至 AlphaGo，一种在 Go 游戏中击败 Lee Sedol 的深度学习程序，都由 TPU 推动。 因此，让我们看看 TPU 到底是什么。

TPU 是 Google 专门为机器学习而定制的定制专用集成电路（**ASIC**），是针对 Tensorflow 量身定制的。 它基于 28 纳米工艺构建，运行频率为 700 MHz，运行时消耗 40 W 的能量。 它包装为外部加速卡，可以插入现有的 SATA 硬盘插槽中。 TPU 通过 PCIe Gen 3×16 总线连接到主机 CPU，该总线提供 12.5 GB/s 的有效带宽。

到目前为止，第一代 TPU 的目标是推理，即使用已经训练好的模型。 DNN 的训练通常需要更多时间，但仍在 CPU 和 GPU 上进行。 在 [2017 年 5 月的博客文章](https://www.blog.google/topics/google-cloud/google-cloud-offer-tpus-machine-learning/)中宣布的第二代 TPU 都可以训练和推断机器学习模型。

# TPU 的组件

在本书涵盖的所有深度学习模型中，无论学习范例如何，都需要进行三个基本计算：乘法，加法和激活函数的应用。

前两个成分是矩阵乘法的一部分：权重矩阵`W`需要与输入矩阵`X`相乘`W^T · X`； 矩阵乘法在 CPU 上的计算量很大，尽管 GPU 使操作并行化，但仍有改进的余地。

TPU 具有 65,536 个 8 位整数矩阵乘法器单元（**MXU**），峰值吞吐量为 92 TOPS。 GPU 和 TPU 乘法之间的主要区别在于 GPU 包含浮点乘法器，而 TPU 包含 8 位整数乘法器。 TPU 还包含一个统一缓冲区（**UB**），用作寄存器的 24 MB SRAM 和一个包含硬接线激活函数的激活单元（**AU**）。

MXU 是使用脉动阵列架构实现的。 它包含一个阵列算术逻辑单元（ALU），该阵列连接到网状拓扑中的少量最近邻居。 每个数据值仅读取一次，但在流过 ALU 数组时会多次用于不同的操作，而无需将其存储回寄存器。 TPU 中的 ALU 仅以固定模式执行乘法和加法。 MXU 已针对矩阵乘法进行了优化，不适用于通用计算。

每个 TPU 还具有一个片外 8GiB DRAM 池，称为加权存储器。 它具有四个阶段的流水线，并执行 CISC 指令。 到目前为止，TPU 由六个神经网络组成：两个 MLP，两个 CNN 和两个 LSTM。

在高级指令的帮助下对 TPU 进行编程； 下面是一些用于对 TPU 进行编程的指令：

*   `Read_Weights`：从内存读取权重
*   `Read_Host_Memory`：从内存中读取数据
*   `MatrixMultiply/Convolve`：与数据相乘或卷积并累加结果
*   `Activate`：应用激活函数
*   `Write_Host_Memory`：将结果写入存储器

Google 创建了一个 API 堆栈，以方便 TPU 编程； 它将来自 Tensorflow 图的 API 调用转换为 TPU 指令。

# TPU 的优势

TPU 提供的优于 GPU 和 CPU 的首要优势是性能。 Google 将 TPU 的性能与运行基准代码（代表 95% 的推理工作量）的服务器级 Intel Haswell CPU 和 NVIDIA K80 GPU 进行了比较。 它发现 TPU 的速度比 NVIDIA GPU 和 Intel CPU 快 15-30 倍。

第二个重要参数是功耗。 降低功耗非常重要，因为它具有双重能源优势：它不仅减少了功耗，而且还通过降低散热成本来散热，从而节省了功耗，从而消除了加工过程中产生的热量。 TPU / CPU 每瓦性能比其他 CPU 和 GPU 配置提高了 30-80 倍。

TPU 的另一个优点是其最小化和确定性的设计，因为它们一次只能执行一个任务。

As compared to CPUs and GPUs, the single-threaded TPU has none of the sophisticated microarchitectural features that consume transistors and energy to improve the average case but not the 99th-percentile case: no caches, branch prediction, out-of-order execution, multiprocessing, speculative prefetching, address coalescing, multithreading, context switching, and so forth. Minimalism is a virtue of domain-specific processors.

# 访问 TPU

Google 已决定不直接将 TPU 出售给他人； 取而代之的是，将通过 Google 云平台提供 TPU：[Cloud TPU Alpha](https://cloud.google.com/tpu/)。 Cloud TPU Alpha 将提供高达 180 teraflops 的计算性能和 64 GB 的超高带宽内存。 用户将能够从自定义虚拟机连接到这些 Cloud TPU。

Google 还决定向全球的机器学习研究人员免费提供 1000 个云 TPU 集群，以加快开放式机器学习研究的步伐。 在有限的计算时间内，将授予选定的个人访问权限； [个人可以使用以下链接进行注册](https://services.google.com/fb/forms/tpusignup/)。 根据 Google Blog：

"Since the main goal of the TensorFlow Research Cloud is to benefit the open machine learning research community as a whole, successful applicants will be expected to do the following:
Share their TFRC-supported research with the world through peer-reviewed publications, open-source code, blog posts, or other open media
Share concrete, constructive feedback with Google to help us improve the TFRC program and the underlying Cloud TPU platform over time.
Imagine a future in which ML acceleration is abundant and develop new kinds of machine learning models in anticipation of that future"

# TPU 上的资源

*   Norman P.Jouppi 等人，张量处理单元的数据中心内性能分析，arXiv：1704.04760（2017）。 在本文中，作者将 TPU 与服务器级的 Intel Haswell CPU 和 NVIDIA k80 GPU 进行了比较。 本文以 TPU 与 CPU 和 K80 GPU 的性能为基准。
*   [此 Google 博客通过以下简单术语说明了 TPU 及其工作原理](https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)