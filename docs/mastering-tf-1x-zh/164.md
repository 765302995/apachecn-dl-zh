# 自编码器类型

自编码器架构可以在各种配置中找到，例如简单自编码器，稀疏自编码器，去噪自编码器和卷积自编码器。

*   **简单自编码器：**在简单的自编码器中，与输入相比，隐藏层具有较少数量的节点或神经元。例如，在 MNIST 数据集中，784 个特征的输入可以连接到 512 个节点的隐藏层或 256 个节点，其连接到 784 特征输出层。因此，在训练期间，仅由 256 个节点学习 784 个特征。 简单自编码器也称为欠完整自编码器。

    简单的自编码器可以是单层或多层。通常，单层自编码器在生产中表现不佳。多层自编码器具有多个隐藏层，分为编码器和解码器分组。编码器层将大量特征编码为较少数量的神经元，然后解码器层将学习的压缩特征解码回原始特征或减少数量的特征。多层自编码器被称为**栈式自编码器** 。
*   **稀疏自编码器**：在稀疏自编码器中，添加正则化项作为惩罚，因此，与简单自编码器相比，表示变得更稀疏。
*   **去噪自编码器**（DAE）：在 DAE 架构中，输入带有随机噪声。 DAE 重新创建输入并尝试消除噪音。 DAE 中的损失函数将去噪重建输出与原始未损坏输入进行比较。
*   **卷积自编码器**（CAE）：前面讨论过的自编码器使用全连接层，这种模式类似于多层感知机模型。我们也可以使用卷积层而不是完全连接或密集层。当我们使用卷积层来创建自编码器时，它被称为卷积自编码器。作为一个例子，我们可以为 CAE 提供以下层：

    **输入 - &gt;卷积 - &gt;池化 - &gt;卷积 - &gt;池化 - &gt;输出

    **第一组卷积和池化层充当编码器，将高维输入特征空间减少到低维特征空间。第二组卷积和池化层充当解码器，将其转换回高维特征空间。

*   **变分自编码器**（VAE）：变分自编码器架构是自编码器领域的最新发展。 VAE 是一种生成模型，即它产生概率分布的参数，从中可以生成原始数据或与原始数据非常相似的数据。

    在 VAE 中，编码器将输入样本转换为潜在空间中的参数，使用该参数对潜在点进行采样。然后解码器使用潜点重新生成原始输入数据。因此，在 VAE 中学习的重点转移到最大化输入数据的概率，而不是试图从输入重建输出。

现在让我们在以下部分中在 TensorFlow 和 Keras 中构建自编码器。我们将使用 MNIST 数据集来构建自编码器。自编码器将学习表示具有较少数量的神经元或特征的 MNIST 数据集的手写数字。

您可以按照 Jupyter 笔记本中的代码`ch-10_AutoEncoders_TF_and_Keras`。

像往常一样，我们首先使用以下代码读取 MNIST 数据集：

```py
from tensorflow.examples.tutorials.mnist.input_data import input_data
dataset_home = os.path.join(datasetslib.datasets_root,'mnist') 
mnist = input_data.read_data_sets(dataset_home,one_hot=False)

X_train = mnist.train.images
X_test = mnist.test.images
Y_train = mnist.train.labels
Y_test = mnist.test.labels

pixel_size = 28
```

我们从训练和测试数据集中提取四个不同的图像及其各自的标签：

```py
while True:
     train_images,train_labels = mnist.train.next_batch(4)
     if len(set(train_labels))==4:
        break
while True:
     test_images,test_labels = mnist.test.next_batch(4)
     if len(set(test_labels))==4:
        break
```

现在让我们看看使用 MNIST 数据集构建自编码器的代码。

您可以按照 Jupyter 笔记本中的代码`ch-10_AutoEncoders_TF_and_Keras`。