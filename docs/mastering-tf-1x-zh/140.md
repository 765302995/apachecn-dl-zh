# 词向量表示

为了从文本数据中学习神经网络模型的参数，首先，我们必须将文本或自然语言数据转换为可由神经网络摄取的格式。神经网络通常以数字向量的形式摄取文本。将原始文本数据转换为数字向量的算法称为字嵌入算法。

一种流行的字嵌入方法是我们在 MNIST 图像分类中看到的**单热编码**。假设我们的文本数据集由 60,000 个字典单词组成。然后，每个单词可以由具有 60,000 个元素的单热编码向量表示，其中除了表示具有值 1 的该单词的一个元素之外，所有其他元素具有零值。

然而，单热编码方法有其缺点。首先，对于具有大量单词的词汇，单热词向量的维数变得非常大。其次，人们无法找到与单热编码向量的单词相似性。例如，假设猫和小猫的向量分别为`[1 0 0 0 0 0]`和`[0 0 0 0 0 1]`。这些向量没有相似之处。

还有其他基于语料库的方法，用于将基于文本的语料库转换为数字向量，例如：

*   术语频率 - 反向文档频率（TF-IDF）
*   潜在语义分析（LSA）
*   主题建模

最近，用数值向量表示单词的焦点已转移到基于分布假设的方法，这意味着具有相似语义含义的单词倾向于出现在类似的上下文中。

两种最广泛使用的方法称为 word2vec 和 GloVe。我们将在本章中使用 word2vec 进行练习。正如我们在前一段中所了解到的，单热编码给出了语料库字典中单词总数大小的维数。使用 word2vec 创建的单词向量的维度要低得多。

word2vec 系列模型使用两种架构构建：

*   **连续词汇**：训练模型以学习给定上下文词的中心词的概率分布。因此，给定一组上下文单词，模型以您在高中语言课程中所做的填空方式预测中心单词。 CBOW 体系结构最适用于具有较小词汇表的数据集。
*   **Skip-gram** ：训练模型以学习给定中心词的上下文词的概率分布。因此，给定一个中心词，模型以您在高中语言课程中完成的句子方式预测语境词。

例如，让我们考虑一下这句话：

```
Vets2data.org is a non-profit for educating the US Military Veterans Community on Artificial Intelligence and Data Science.
```

在 CBOW 架构中，给出单词`Military`和`Community`，模型学习单词`Veterans`的概率，并在 skip-gram 架构中，给出单词 `Veterans`，模型学习单词`Military`和`Community`的概率。

word2vec 模型以无监督的方式从文本语料库中学习单词向量。文本语料库分为成对的上下文单词和目标单词。虽然这些对是真正的对，但是伪对是用随机配对的上下文词和上下文词生成的，因此在数据中产生噪声。训练分类器以学习用于区分真对和假对的参数。该分类器的参数成为 word2vec 模型或单词向量。

关于 word2vec 理论背后的数学和理论的更多信息可以从以下论文中学到：

```
Mikolov, T., I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and Their Compositionality. _Advances in Neural Information Processing Systems_, 2013, pp. 3111–3119.

Mikolov, T., K. Chen, G. Corrado, and J. Dean. Efficient Estimation of Word Representations in Vector Space. _arXiv_, 2013, pp. 1–12.

Rong, X. word2vec Parameter Learning Explained. _arXiv:1411.2738_, 2014, pp. 1–19.

Baroni, M., G. Dinu, and G. Kruszewski. Don’t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors. 2014.
```

您应该使用 GloVe 和 word2vec 练习并应用适用于您的文本数据的方法。

有关 GLoVe 算法的更多信息可以从以下文章中学习：

```
Pennington, J., R. Socher, and C. Manning. GloVe: Global Vectors for Word Representation. 2014.
```

让我们通过在 TensorFlow 和 Keras 中创建单词向量来理解 word2vec 模型。

您可以按照 Jupyter 笔记本中的下几节的代码`ch-08a_Embeddings_in_TensorFlow_and_Keras`。
