# 跨计算设备执行图 - CPU 和 GPU

图可以分为多个部分，每个部分可以放置在不同的设备上执行，例如 CPU 或 GPU。您可以使用以下命令列出可用于执行图的所有设备：

```py
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```

我们得到以下输出（您的输出会有所不同，具体取决于系统中的计算设备）：

```py
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 12900903776306102093
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 611319808
locality {
  bus_id: 1
}
incarnation: 2202031001192109390
physical_device_desc: "device: 0, name: Quadro P5000, pci bus id: 0000:01:00.0, compute capability: 6.1"
]
```

TensorFlow 中的设备用字符串  `/device:&lt;device_type&gt;:&lt;device_idx&gt;`标识。在上述输出中，  `CPU` 和  `GPU` 表示器件类型，`0`表示器件索引。

关于上述输出需要注意的一点是它只显示一个 CPU，而我们的计算机有 8 个 CPU。原因是 TensorFlow 隐式地在 CPU 单元中分配代码，因此默认情况下  `CPU:0` 表示 TensorFlow 可用的所有 CPU。当 TensorFlow 开始执行图时，它在一个单独的线程中运行每个图中的独立路径，每个线程在一个单独的 CPU 上运行。我们可以通过改变  `inter_op_parallelism_threads`的数量来限制用于此目的的线程数。类似地，如果在独立路径中，操作能够在多个线程上运行，TensorFlow 将在多个线程上启动该特定操作。可以通过设置  `intra_op_parallelism_threads`的数量来更改此池中的线程数。