# GRU 网络

LSTM 网络的计算成本很高，因此，研究人员发现了一种几乎同样有效的 RNN 配置，称为**门控递归单元**（ **GRU** ）架构。

在 GRU 中，不使用工作和长期记忆，只使用一种记忆，用 _**h**_（隐藏状态）表示。 GRU 单元通过**复位**和**更新**门，将信息添加到此状态存储器，或从该状态存储器中删除信息。

下图描绘了 GRU 单元（说明如下图）：

![](img/cb5734c0-dc19-4d5d-bf98-eb448f083dd2.png)The GRU Cell

GRU 单元中通过门的内部流量如下：

1.  **更新门`u()`**：输入`h[t-1]`和`x[t]`按照以下公式流向`u()`门：
    
    ![](img/772bddac-9209-47d8-a1c4-eb66fe59fe1f.png)

2.  **复位门`r()`**：输入`h[t-1]`和`x[t]`按照以下公式流向`r()`门：
    
    ![](img/a0f59f6c-94df-4742-bd5f-5ac3d81540cf.png)

1.  **候选状态记忆**：候选长期记忆是根据`r()`门，`h[t-1]`和`x[t]`的输出计算出来的，按照下列公式：
    
    ![](img/7ce40c11-c131-493c-8ccf-7759863540c0.png)

2.  接下来，组合前面的三个计算以得到更新的状态存储器，由`h[t]`，表示，如下式所示：
    
    ![](img/519ccb8b-513c-4b1a-a1d7-9f48d6c2577b.png)

阅读以下研究论文以探索 GRU 的更多细节：

```
K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014. https://arxiv.org/abs/1406.1078

J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, pp. 1–9, 2014. https://arxiv.org/abs/1412.3555
```