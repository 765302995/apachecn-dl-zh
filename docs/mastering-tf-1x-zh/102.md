# 总结

在本章中，我们学习了如何在 TensorFlow 中应用经典机器学习算法，而不使用神经网络。在本章的第一部分，我们了解了回归模型。我们解释了如何训练具有一个或多个特征的线性回归模型。我们使用 TensorFlow 编写线性回归代码。我们还讨论了正则化基本上是增加一个惩罚项，以便模型在训练阶段学习参数时不会过拟合训练数据。我们使用 TensorFlow 实现了 Lasso，Ridge 和 ElasticNet 正则化。 TensorFlow 有一些内置的正则化方法，我们将在下一章中学习。

在本章的后续章节中，我们了解了有监督机器学习中的分类问题。我们讨论了两类和多类分类的模型函数，平滑函数和损失函数。我们在本章中使用了逻辑回归，因为这是实现分类的最简单方法。对于二分类，我们使用 sigmoid 函数，对于多类分类，我们使用 softmax 函数来平滑线性模型的值，以产生输出在特定类中的概率。

我们在 TensorFlow 中实现了模型和损失函数的逻辑，并训练模型进行二分类和多类分类。虽然我们在本章中使用了经典的机器学习方法，并使用 TensorFlow 实现了它们，但是当我们实现神经网络和深度神经网络来解决机器学习问题时，TensorFlow 的全部功能得以释放。我们将在本书的神经网络相关章节中研究这些高级方法。

建议您阅读以下书籍以了解有关回归和分类的更多详细信息：

Sebastian Raschka，Python 机器学习，第 2 版，Packt Publishing，2017

Trevor Hastie，Robert Tibshirani，Jerome Friedman，统计学习的要素，第二版，施普林格，2013