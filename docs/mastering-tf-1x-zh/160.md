# 使用 TensorFlow 的用于 CIFAR10 的 ConvNets

我们保持层，滤波器及其大小与之前的 MNIST 示例中的相同，增加了一个正则化层。由于此数据集与 MNIST 相比较复杂，因此我们为正则化目的添加了额外的 dropout 层：

```py
tf.nn.dropout(layer1_pool, keep_prob)
```

在预测和评估期间，占位符`keep_prob`设置为 1。这样我们就可以重复使用相同的模型进行培训以及预测和评估。

有关 CIFAR10 数据的 LeNet 模型的完整代码在笔记本 `ch-09b_CNN_CIFAR10_TF_and_Keras` 中提供。

在运行模型时，我们得到以下输出：

```py
Epoch: 0000   loss = 2.115784
Epoch: 0001   loss = 1.620117
Epoch: 0002   loss = 1.417657
Epoch: 0003   loss = 1.284346
Epoch: 0004   loss = 1.164068
Epoch: 0005   loss = 1.058837
Epoch: 0006   loss = 0.953583
Epoch: 0007   loss = 0.853759
Epoch: 0008   loss = 0.758431
Epoch: 0009   loss = 0.663844
Epoch: 0010   loss = 0.574547
Epoch: 0011   loss = 0.489902
Epoch: 0012   loss = 0.410211
Epoch: 0013   loss = 0.342640
Epoch: 0014   loss = 0.280877
Epoch: 0015   loss = 0.234057
Epoch: 0016   loss = 0.195667
Epoch: 0017   loss = 0.161439
Epoch: 0018   loss = 0.140618
Epoch: 0019   loss = 0.126363
Model Trained.
Accuracy: 0.6361
```

与我们在 MNIST 数据上获得的准确率相比，我们没有获得良好的准确性。通过调整不同的超参数并改变卷积和池化层的组合，可以实现更好的准确性。我们将其作为挑战，让读者探索并尝试不同的 LeNet 架构和超参数变体，以实现更高的准确性。