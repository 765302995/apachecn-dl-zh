# 定义优化器函数

接下来，我们实例化学习率为 0.001 的`theGradientDescentOptimizer`函数并将其设置为最小化损失函数：

```py
learning_rate = 0.001
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
```

有关梯度下降的更多详细信息，请访问此链接：

<https://en.wikipedia.org/wiki/Gradient_descent>

<https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/>

TensorFlow 提供了许多其他优化器函数，如 Adadelta，Adagrad 和 Adam。我们将在以下章节中介绍其中一些内容。