# TensorFlow GPU 设置

要将  TensorFlow 与 NVIDIA GPU 配合使用，第一步是安装 CUDA Toolkit。

### 注意

要了解更多信息，请访问[此链接](https://developer.nvidia.com/cuda-downloads)。

安装 CUDA Toolkit 后，必须从[此链接](https://developer.nvidia.com/cudnn)下载适用于 Linux 的 cuDNN v5.1 库。

cuDNN 是一个有助于加速深度学习框架的库，例如 TensorFlow 和 Theano。以下是 NVIDIA 网站的简要说明：

> “NVIDIACUDA®深度神经网络库（cuDNN）是用于深度神经网络的 GPU 加速原语库.cuDNN 为标准例程提供高度调整的实现，例如前向和后向卷积，池化，正则化和激活层.cuDNN is NVIDIA 深度学习 SDK 的一部分。“

在安装之前，您需要在 NVIDIA 的加速计算开发人员计划中注册。注册后，登录并将 cuDNN 5.1 下载到本地计算机。

下载完成后，解压缩文件并将其复制到 CUDA Toolkit 目录中（我们假设目录为`/usr/local/cuda/`）：

```py
$ sudo tar -xvf cudnn-8.0-linux-x64-v5.1-rc.tgz -C /usr/local

```

## 更新 TensorFlow

我们假设您将使用 TensorFlow 来构建您的深度神经网络模型。只需通过 pip 用`upgrade`标志更新 TensorFlow。

我们假设您当前正在使用 TensorFlow 0.11：

```py
pip install — upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl

```

现在，您应该拥有使用 GPU 运行模型所需的一切。

## GPU 表示

在 TensorFlow 中，  支持的设备表示为字符串：

*   `"/cpu:0"`：您机器的 CPU
*   `"/gpu:0"`：机器的 GPU，如果你有的话
*   `"/gpu:1"`：你机器的第二个 GPU，依此类推

当将操作分配给 GPU 设备时，  执行流程给予优先级。

## 使用 GPU

要在 TensorFlow 程序中使用  GPU，只需键入以下内容：

```py
with tf.device("/gpu:0"):
```

然后你需要进行设置操作。这行代码将创建一个新的上下文管理器，告诉 TensorFlow 在 GPU 上执行这些操作。

让我们考虑以下示例，其中我们要执行以下两个大矩阵的和：`A^n + B^n`。

定义基本导入：

```py
import numpy as np
import tensorflow as tf
import datetime
```

我们可以配置 TensorFlow 程序，以找出您的操作和张量分配给哪些设备。为此，我们将创建一个会话，并将以下`log_device_placement`参数设置为`True`：

```py
log_device_placement = True
```

然后我们设置`n`参数，这是要执行的乘法次数：

```py
n=10
```

然后我们构建两个随机大矩阵。我们使用 NumPy `rand`函数来执行此操作：

```py
A = np.random.rand(10000, 10000).astype('float32')
B = np.random.rand(10000, 10000).astype('float32')
```

`A`和`B`各自的大小为 10000x10000。

以下数组将用于存储结果：

```py
c1 = []
c2 = []
```

接下来，我们定义将由 GPU 执行的内核矩阵乘法函数：

```py
def matpow(M, n):
    if n == 1:
        return M
    else:
        return tf.matmul(M, matpow(M, n-1))
```

正如我们之前解释的  ，我们必须配置 GPU 和 GPU 以执行以下操作：

GPU 将计算`A^n`和`B^n`操作并将结果存储在`c1`中：

```py
with tf.device('/gpu:0'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))
    c1.append(matpow(b, n))
```

`c1`（`A^n + B^n`）中所有元素的添加由 CPU 执行，因此我们定义以下内容：

```py
with tf.device('/cpu:0'):
  sum = tf.add_n(c1) 
```

`datetime`类允许我们评估计算时间：

```py
t1_1 = datetime.datetime.now()
with tf.Session(config=tf.ConfigProto\
          (log_device_placement=log_device_placement)) as sess:
    sess.run(sum, {a:A, b:B})
t2_1 = datetime.datetime.now()
```

然后显示计算时间：

```py
print("GPU computation time: " + str(t2_1-t1_1))
```

在我的笔记本电脑上，使用 GeForce 840M 显卡，结果如下：

```py
GPU computation time: 0:00:13.816644

```

## GPU 内存管理

在一些情况下，希望该过程仅分配可用内存的子集，或者仅增加该过程所需的内存使用量。 TensorFlow 在会话中提供两个配置选项来控制它。

第一个是`allow_growth`选项，它尝试根据运行时分配仅分配尽可能多的 GPU 内存：它开始分配非常少的内存，并且随着会话运行并需要更多 GPU 内存，我们扩展 GPU 内存量 TensorFlow 流程需要。

请注意，  我们不释放内存，因为这可能导致更糟糕的内存碎片。要打开此选项，请在`ConfigProto`中设置选项，如下所示：

```py
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config, ...)
```

第二种方法是`per_process_gpu_memory_fraction`选项，它确定应分配每个可见 GPU 的总内存量的分数。例如，您可以告诉 TensorFlow 仅分配每个 GPU 总内存的 40％，如下所示：

```py
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)
```

如果要真正限制 TensorFlow 进程可用的 GPU 内存量，这非常有用。

## 在多 GPU 系统上分配单个 GPU

如果系统中有多个 GPU，则默认情况下将选择 ID 最低的 GPU。如果您想在不同的 GPU 上运行会话，则需要明确指定首选项。

例如，我们可以尝试更改上一代码中的 GPU 分配：

```py
with tf.device('/gpu:1'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))
    c1.append(matpow(b, n))
```

这样，我们告诉 gpu1 执行内核函数。如果我们指定的设备不​​存在（如我的情况），您将获得`InvalidArgumentError`：

```py
InvalidArgumentError (see above for traceback): Cannot assign a device to node 'Placeholder_1': Could not satisfy explicit device specification '/device:GPU:1' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
     [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[100,100], _device="/device:GPU:1"]()]]

```

如果您希望 TensorFlow 自动选择现有且受支持的设备来运行操作（如果指定的设备不​​存在），则可以在创建会话时在配置选项中将`allow_soft_placement`设置为 True。

我们再次为以下节点设置`'/gpu:1'`：

```py
with tf.device('/gpu:1'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))
    c1.append(matpow(b, n))
```

然后我们构建一个`Session`，并将以下`allow_soft_placement`参数设置为`True`：

```py
with tf.Session(config=tf.ConfigProto\
                 (allow_soft_placement=True,\
                log_device_placement=log_device_placement))\
                  as sess:
```

这样，当我们运行会话时，不会显示`InvalidArgumentError`。在这种情况下，我们会得到一个正确的结果，但有一点延迟：

```py
GPU computation time: 0:00:15.006644

```

## 具有软放置的 GPU 的源代码

这是完整源代码，仅为了清楚起见：

```py
import numpy as np
import tensorflow as tf
import datetime

log_device_placement = True
n = 10

A = np.random.rand(10000, 10000).astype('float32')
B = np.random.rand(10000, 10000).astype('float32')

c1 = []
c2 = []

def matpow(M, n):
    if n == 1: 
        return M
    else:
        return tf.matmul(M, matpow(M, n-1))

with tf.device('/gpu:0'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))
    c1.append(matpow(b, n))

with tf.device('/cpu:0'):
    sum = tf.add_n(c1) 

t1_1 = datetime.datetime.now()
with tf.Session(config=tf.ConfigProto\
                 (allow_soft_placement=True,\
                log_device_placement=log_device_placement))\
                  as sess:
     sess.run(sum, {a:A, b:B})
t2_1 = datetime.datetime.now()
```

## 使用多个 GPU

如果您想在多个 GPU 上运行 TensorFlow，您可以通过为 GPU 分配特定的代码块来构建模型。例如，如果我们有两个 GPU，我们可以按如下方式拆分前面的代码，将第一个矩阵计算分配给第一个 GPU：

```py
with tf.device('/gpu:0'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))
```

第二个矩阵计算分配给第二个 GPU：

```py
with tf.device('/gpu:1'):
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(b, n))
```

CPU 将管理结果。另请注意，我们使用共享`c1`数组来收集它们：

```py
with tf.device('/cpu:0'):
    sum = tf.add_n(c1)
```

在下面的代码片段中，我们提供了两个 GPU 管理的具体示例：

```py
import numpy as np
import tensorflow as tf
import datetime

log_device_placement = True
n = 10

A = np.random.rand(10000, 10000).astype('float32')
B = np.random.rand(10000, 10000).astype('float32')

c1 = []

def matpow(M, n):
    if n == 1:  
        return M
    else:
        return tf.matmul(M, matpow(M, n-1))

#FIRST GPU
with tf.device('/gpu:0'):
    a = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(a, n))

#SECOND GPU
with tf.device('/gpu:1'):
    b = tf.placeholder(tf.float32, [10000, 10000])
    c1.append(matpow(b, n))

with tf.device('/cpu:0'):
    sum = tf.add_n(c1) 

t1_1 = datetime.datetime.now()
with tf.Session(config=tf.ConfigProto\
                 (allow_soft_placement=True,\
                log_device_placement=log_device_placement))\
                  as sess:
     sess.run(sum, {a:A, b:B})
t2_1 = datetime.datetime.now()
```