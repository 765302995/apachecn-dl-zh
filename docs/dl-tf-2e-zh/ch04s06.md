# 微调实现

我们的分类任务包含两个类别，因此网络的新 softmax 层将包含 2 个类别而不是 1,000 个类别。这是输入张量，它是一个 227×227×3 图像，以及等级 2 的输出张量：

```py
n_classes = 2
train_x = zeros((1, 227,227,3)).astype(float32)
train_y = zeros((1, n_classes))
```

微调实现包括截断预训练网络的最后一层（softmax 层），并将其替换为与我们的问题相关的新 softmax 层。

例如，ImageNet 上预先训练好的网络带有一个包含 1,000 个类别的 softmax 层。

以下代码片段定义了新的 softmax 层`fc8`：

```py
fc8W = tf.Variable(tf.random_normal\
                   ([4096, n_classes]),\
                   trainable=True, name="fc8w")
fc8b = tf.Variable(tf.random_normal\
                   ([n_classes]),\
                   trainable=True, name="fc8b")
fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)
prob = tf.nn.softmax(fc8)
```

损失是用于分类的表现指标。它是一个始终为正的连续函数，如果模型的预测输出与期望的输出完全匹配，则交叉熵等于零。因此，优化的目标是通过改变模型的权重和偏差来最小化交叉熵，因此它尽可能接近零。

TensorFlow 具有用于计算交叉熵的内置函数。为了使用交叉熵来优化模型的变量，我们需要一个标量值，因此我们只需要对所有图像分类采用交叉熵的平均值：

```py
loss = tf.reduce_mean\
       (tf.nn.softmax_cross_entropy_with_logits_v2\
        (logits =prob, labels=y))
opt_vars = [v for v in tf.trainable_variables()\
            if (v.name.startswith("fc8"))]
```

既然我们必须最小化成本度量，那么我们可以创建`optimizer`：

```py
optimizer = tf.train.AdamOptimizer\
            (learning_rate=learning_rate).minimize\
            (loss, var_list = opt_vars)
correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
```

在这种情况下，我们使用步长为`0.5`的`AdamOptimizer`。请注意，此时不执行优化。事实上，根本没有计算任何东西，我们只需将优化器对象添加到 TensorFlow 图中以便以后执行。然后我们在网络上运行反向传播以微调预训练的权重：

```py
batch_size = 100
training_iters = 6000
display_step = 1
dropout = 0.85 # Dropout, probability to keep units

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    step = 1
```

继续训练，直到达到最大迭代次数：

```py
    while step * batch_size < training_iters:
        batch_x, batch_y = \
                 next(next_batch(batch_size)) #.next()
```

运行优化操作（反向传播）：

```py
        sess.run(optimizer, \
                 feed_dict={x: batch_x, \
                            y: batch_y, \
                            keep_prob: dropout})

        if step % display_step == 0:
```

计算批次损失和准确率：

```py
            cost, acc = sess.run([loss, accuracy],\
                                 feed_dict={x: batch_x, \
                                            y: batch_y, \
                                            keep_prob: 1.})
            print ("Iter " + str(step*batch_size) \
                   + ", Minibatch Loss= " + \
                  "{:.6f}".format(cost) + \
                   ", Training Accuracy= " + \
                  "{:.5f}".format(acc))              

        step += 1
    print ("Optimization Finished!")
```

网络训练产生以下结果：

```py
Iter 100, Minibatch Loss= 0.555294, Training Accuracy= 0.76000
Iter 200, Minibatch Loss= 0.584999, Training Accuracy= 0.73000
Iter 300, Minibatch Loss= 0.582527, Training Accuracy= 0.73000
Iter 400, Minibatch Loss= 0.610702, Training Accuracy= 0.70000
Iter 500, Minibatch Loss= 0.583640, Training Accuracy= 0.73000
Iter 600, Minibatch Loss= 0.583523, Training Accuracy= 0.73000
…………………………………………………………………
…………………………………………………………………
Iter 5400, Minibatch Loss= 0.361158, Training Accuracy= 0.95000
Iter 5500, Minibatch Loss= 0.403371, Training Accuracy= 0.91000
Iter 5600, Minibatch Loss= 0.404287, Training Accuracy= 0.91000
Iter 5700, Minibatch Loss= 0.413305, Training Accuracy= 0.90000
Iter 5800, Minibatch Loss= 0.413816, Training Accuracy= 0.89000
Iter 5900, Minibatch Loss= 0.413476, Training Accuracy= 0.90000
Optimization Finished!

```

要测试我们的模型，我们将预测与标签集（cat = 0，dog = 1）进行比较：

```py
    output = sess.run(prob, feed_dict = {x:imlist, keep_prob: 1.})
    result = np.argmax(output,1)
    testResult = [1,1,1,1,0,0,0,0,0,0,\
                  0,1,0,0,0,0,1,1,0,0,\
                  1,0,1,1,0,1,1,0,0,1,\
                  1,1,1,0,0,0,0,0,1,0,\
                  1,1,1,1,0,1,0,1,1,0,\
                  1,0,0,1,0,0,1,1,1,0,\
                  1,1,1,1,1,0,0,0,0,0,\
                  0,1,1,1,0,1,1,1,1,0,\
                  0,0,1,0,1,1,1,1,0,0,\
                  0,0,0,1,1,0,1,1,0,0]
    count = 0
    for i in range(0,99):
        if result[i] == testResult[i]:
            count=count+1

    print("Testing Accuracy = " + str(count) +"%")
```

最后，我们有我们模型的准确率：

```py
Testing Accuracy = 82%
```

## VGG

VGG 是在 2014 年 ILSVRC 期间发明神经网络的人的名字。我们谈论的是复数网络，因为创建了同一网络的多个版本，每个拥有不同数量的层。根据层数`n`，这些网络中的一个具有的权重，它们中的每一个通常称为 VGG-n。所有这些网络都比 AlexNet 更深。这意味着它们由多个层组成，其参数比 AlexNet 更多，在这种情况下，总共有 11 到 19 个训练层。通常，只考虑可行的层，因为它们会影响模型的处理和大小，如前一段所示。然而，整体结构仍然非常相似：总是有一系列初始卷积层和最后一系列完全连接的层，后者与 AlexNet 完全相同。因此，使用的卷积层的数量，当然还有它们的参数有什么变化。下表显示了 VGG 团队构建的所有变体。

每一列，从左侧开始，向右侧，显示一个特定的 VGG 网络，从最深到最浅。粗体项表示与先前版本相比，每个版本中添加的内容。 ReLU 层未在表中显示，但在网络中它存在于每个卷积层之后。所有卷积层使用 1 的步幅：

![VGG](img/B09698_04_11.jpg)

表：VGG 网络架构

请注意，  AlexNet 没有具有相当大的感受野的卷积层：这里，所有感受野都是 3×3，除了 VGG-16 中有几个具有 1×1 感受野的卷积层。回想一下，具有 1 步梯度的凸层不会改变输入空间大小，同时修改深度值，该深度值与使用的内核数量相同。因此，VGG 卷积层不会影响输入体积的宽度和高度;只有池化层才能这样做。使用具有较小感受野的一系列卷积层的想法最终总体上模拟具有较大感受野的单个卷积层，这是由于这样的事实，即以这种方式使用多个 ReLU 层而不是单独使用一个，从而增加激活函数的非线性，从而使其更具区别性。它还用于减少使用的参数数量。这些网络被认为是 AlexNet 的演变，因为总体而言，使用相同的数据集，它们的表现优于 AlexNet。 VGG 网络演示的主要概念是拥塞神经网络越来越深刻，其表现也越来越高。但是，  必须拥有越来越强大的硬件，否则网络训练就会成为问题。

对于 VGG，使用了四个 NVIDIA Titan Blacks，每个都有 6 GB 的内存。因此，VGG 具有更好的表现，但需要大量的硬件用于训练，并且还使用大量参数：例如，VGG-19 模型大约为 550 MB（是 AlexNet 的两倍）。较小的 VGG 网络仍然具有大约 507 MB 的模型。

## 用 VGG-19 学习艺术风格

在这个项目中，我们使用预训练的 VGG-19 来学习艺术家创建的样式和模式，并将它们转移到图像中（项目文件是`style_transfer.py`，在本书的 GitHub 仓库中） ）。这种技术被称为`artistic style learning`（[参见 Gatys 等人的文章 A Art Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)）。根据学术文献，艺术风格学习定义如下：给定两个图像作为输入，合成具有第一图像的语义内容和第二图像的纹理/风格的第三图像。

为了使其正常工作，我们需要训练一个深度卷积神经网络来构建以下内容：

*   用于确定图像 A 的内容的内容提取器
*   用于确定图像 B 样式的样式提取器
*   合并器将一些任意内容与另一个任意样式合并，以获得最终结果

    ![Artistic style learning with VGG-19](img/B09698_04_12.jpg)

    图 11：艺术风格学习操作模式

## 输入图像

输入图像，每个都是 478×478 像素，是您在本书的代码库中也可以找到的以下图像（`cat.jpg`和`mosaic.jpg`）：

![Input images](img/B09698_04_13.jpg)

图 12：艺术风格学习中的输入图像

为了通过 VGG 模型分析  ，需要对这些图像进行预处理：

1.  添加额外的维度
2.  从输入图像中减去`MEAN_VALUES`：

    ```py
    MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))
    content_image = preprocess('cat.jpg')
    style_image = preprocess('mosaic.jpg') 

    def preprocess(path):
        image = plt.imread(path)
        image = image[np.newaxis]
        image = image - MEAN_VALUES
        return image
    ```

## 内容提取器和损失

为了隔离图像的语义内容，我们使用预先训练好的 VGG-19 神经网络，对权重进行了一些微调，以适应这个问题，然后使用其中一个隐藏层的输出作为内容提取器。下图显示了用于此问题的 CNN：

![Content extractor and loss](img/B09698_04_14.jpg)

图 13：用于艺术风格学习的 VGG-19

使用以下代码加载预训练的  VGG：

```py
import scipy.io
vgg = scipy.io.loadmat('imagenet-vgg-verydeep-19.mat')
```

`imagenet-vgg-verydeep-19.mat`模型应从[此链接](http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat)下载。

该模型有 43 层，其中 19 层是卷积层。其余的是最大池/激活/完全连接的层。

我们可以检查每个卷积层的形状：

```py
 [print (vgg_layers[0][i][0][0][2][0][0].shape,\
        vgg_layers[0][i][0][0][0][0]) for i in range(43) 
 if 'conv' in vgg_layers[0][i][0][0][0][0] \
 or 'fc' in vgg_layers[0][i][0][0][0][0]]
```

上述代码的结果如下：

```py
(3, 3, 3, 64) conv1_1
(3, 3, 64, 64) conv1_2
(3, 3, 64, 128) conv2_1
(3, 3, 128, 128) conv2_2
(3, 3, 128, 256) conv3_1
(3, 3, 256, 256) conv3_2
(3, 3, 256, 256) conv3_3
(3, 3, 256, 256) conv3_4
(3, 3, 256, 512) conv4_1
(3, 3, 512, 512) conv4_2
(3, 3, 512, 512) conv4_3
(3, 3, 512, 512) conv4_4
(3, 3, 512, 512) conv5_1
(3, 3, 512, 512) conv5_2
(3, 3, 512, 512) conv5_3
(3, 3, 512, 512) conv5_4
(7, 7, 512, 4096) fc6
(1, 1, 4096, 4096) fc7
(1, 1, 4096, 1000) fc8

```

每种形状以下列方式表示：`[kernel height, kernel width, number of input channels, number of output channels]`。

第一层有 3 个输入通道，因为输入是 RGB 图像，而卷积层的输出通道数从 64 到 512，所有内核都是 3x3 矩阵。

然后我们应用转移学习技术，以使 VGG-19 网络适应我们的问题：

1.  不需要完全连接的层，因为它们用于对象识别。
2.  最大池层代替平均池层，以获得更好的结果。平均池层的工作方式与卷积层中的内核相同。

    ```py
    IMAGE_WIDTH = 478
    IMAGE_HEIGHT = 478
    INPUT_CHANNELS = 3
    model = {}
    model['input'] = tf.Variable(np.zeros((1, IMAGE_HEIGHT,\
                                     IMAGE_WIDTH,\
                                     INPUT_CHANNELS)),\
                                   dtype = 'float32')

    model['conv1_1']  = conv2d_relu(model['input'], 0, 'conv1_1')
    model['conv1_2']  = conv2d_relu(model['conv1_1'], 2, 'conv1_2')
    model['avgpool1'] = avgpool(model['conv1_2'])

    model['conv2_1']  = conv2d_relu(model['avgpool1'], 5, 'conv2_1')
    model['conv2_2']  = conv2d_relu(model['conv2_1'], 7, 'conv2_2')
    model['avgpool2'] = avgpool(model['conv2_2'])

    model['conv3_1']  = conv2d_relu(model['avgpool2'], 10, 'conv3_1')
    model['conv3_2']  = conv2d_relu(model['conv3_1'], 12, 'conv3_2')
    model['conv3_3']  = conv2d_relu(model['conv3_2'], 14, 'conv3_3')
    model['conv3_4']  = conv2d_relu(model['conv3_3'], 16, 'conv3_4')
    model['avgpool3'] = avgpool(model['conv3_4'])

    model['conv4_1']  = conv2d_relu(model['avgpool3'], 19,'conv4_1')
    model['conv4_2']  = conv2d_relu(model['conv4_1'], 21, 'conv4_2')
    model['conv4_3']  = conv2d_relu(model['conv4_2'], 23, 'conv4_3')
    model['conv4_4']  = conv2d_relu(model['conv4_3'], 25,'conv4_4')
    model['avgpool4'] = avgpool(model['conv4_4'])

    model['conv5_1']  = conv2d_relu(model['avgpool4'], 28, 'conv5_1')
    model['conv5_2']  = conv2d_relu(model['conv5_1'], 30, 'conv5_2')
    model['conv5_3']  = conv2d_relu(model['conv5_2'], 32, 'conv5_3')
    model['conv5_4']  = conv2d_relu(model['conv5_3'], 34, 'conv5_4')
    model['avgpool5'] = avgpool(model['conv5_4'])
    ```

这里我们定义了`contentloss`函数来测量两个图像`p`和`x`之间的内容差异：

```py
def contentloss(p, x):
    size = np.prod(p.shape[1:])
    loss = (1./(2*size)) * tf.reduce_sum(tf.pow((x - p),2))
    return loss
```

当输入图像在内容方面彼此非常接近并且随着其内容偏离而增长时，该函数倾向于为 0。

我们将在`conv5_4`层上使用`contentloss`。这是输出层，其输出将是预测，因此我们需要使用`contentloss`函数将此预测与实际预测进行比较：

```py
content_loss = contentloss\
               (sess.run(model['conv5_4']), model['conv5_4'])
```

最小化`content_loss`意味着混合图像在给定层中具有与内容图像的激活非常相似的特征激活。

## 样式提取器和损失

样式提取器使用过滤器的  Gram 矩阵作为给定的隐藏层。简单来说，使用这个矩阵，我们可以破坏图像的语义，保留其基本组件并使其成为一个好的纹理提取器：

```py
def gram_matrix(F, N, M):
    Ft = tf.reshape(F, (M, N))
    return tf.matmul(tf.transpose(Ft), Ft)
```

`style_loss`测量两个图像彼此之间的接近程度。此函数是样式图像和输入 `noise_image`生成的 Gram 矩阵元素的平方差的总和：

```py
noise_image = np.random.uniform\
              (-20, 20,\
               (1, IMAGE_HEIGHT, \
                IMAGE_WIDTH,\
                INPUT_CHANNELS)).astype('float32')

def style_loss(a, x):
    N = a.shape[3]
    M = a.shape[1] * a.shape[2]
    A = gram_matrix(a, N, M)
    G = gram_matrix(x, N, M)
    result = (1/(4 * N**2 * M**2))* tf.reduce_sum(tf.pow(G-A,2))
    return result
```

`style_loss`生长  ，因为它的两个输入图像（`a`和`x`）倾向于偏离风格。

## 合并和总损失

我们可以合并内容和样式损失，以便训练输入`noise_image`来输出（在层中）与样式图像类似的样式，其特征相似于内容图像：

```py
alpha = 1
beta = 100
total_loss = alpha * content_loss + beta * styleloss
```

## 训练

最小化网络中的损失，以便样式损失（输出图像的样式和样式图像的样式之间的损失），内容损失（内容图像和输出图像之间的损失），以及总变异损失尽可能低：

```py
train_step = tf.train.AdamOptimizer(1.5).minimize(total_loss)
```

从这样的网络生成的输出图像应该类似于输入图像并且具有样式图像的造型师属性。

最后，我们可以准备网络进行训练：

```py
sess.run(tf.global_variables_initializer())
sess.run(model['input'].assign(input_noise))
for it in range(2001):
    sess.run(train_step)
    if it%100 == 0:
        mixed_image = sess.run(model['input'])
        print('iteration:',it,'cost: ', sess.run(total_loss))
        filename = 'out2/%d.png' % (it)
        deprocess(filename, mixed_image)
```

训练时间可能非常耗时，但结果可能非常有趣：

```py
iteration: 0 cost:  8.14037e+11
iteration: 100 cost:  1.65584e+10
iteration: 200 cost:  5.22747e+09
iteration: 300 cost:  2.72995e+09
iteration: 400 cost:  1.8309e+09
iteration: 500 cost:  1.36818e+09
iteration: 600 cost:  1.0804e+09
iteration: 700 cost:  8.83103e+08
iteration: 800 cost:  7.38783e+08
iteration: 900 cost:  6.28652e+08
iteration: 1000 cost:  5.41755e+08

```

经过 1000 次迭代后，我们创建了一个新的拼接：

![Training](img/B09698_04_15.jpg)

图 14：艺术风格学习中的输出图像

真是太棒了！你终于可以训练你的神经网络像毕加索一样画画......玩得开心！