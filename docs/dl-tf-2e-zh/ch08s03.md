# PrettyTensor

PrettyTensor 允许开发人员包装 TensorFlow 操作，以快速链接任意数量的层来定义神经网络。即将推出的是 Pretty Tensor 功能的简单示例：我们将一个标准的 TensorFlow 对象包装成一个与库兼容的对象;然后我们通过三个完全连接的层提供它，我们最终输出 softmax 分布：

```py
pretty = tf.placeholder([None, 784], tf.float32)
softmax = (prettytensor.wrap(examples)
 .fully_connected(256, tf.nn.relu)
 .fully_connected(128, tf.sigmoid)
 .fully_connected(64, tf.tanh)
 .softmax(10))
```

PrettyTensor 安装非常简单。您只需使用`pip`安装程序：

```py
sudo pip install prettytensor

```

## 链接层

PrettyTensor 具有三种操作模式，共享链式方法的能力。

## 正常模式

在正常模式下，每调用一次，就会创建一个新的 PrettyTensor。这样可以轻松链接，您仍然可以多次使用任何特定对象。这样可以轻松分支网络。

## 顺序模式

在顺序模式下，  内部变量 - 头部 - 跟踪最近的输出张量，从而允许将调用链分解为多个语句。

这是一个简单的例子：

```py
seq = pretty_tensor.wrap(input_data).sequential()
seq.flatten()
seq.fully_connected(200, activation_fn=tf.nn.relu)
seq.fully_connected(10, activation_fn=None)
result = seq.softmax(labels, name=softmax_name))
```

## 分支和连接

可以使用一流`branch`和`join`方法构建复杂网络：

*   `branch`创建一个单独的 PrettyTensor 对象，该对象在调用时指向当前头部，这允许用户定义一个单独的塔，该塔以回归目标结束，以输出结束或重新连接网络。重新连接允许用户定义复合层，如初始。
*   `join`用于连接多个输入或重新连接复合层。

## 数字分类器

在这个例子中，我们将定义和训练一个两层模型和一个 LeNe​​t 5 风格的卷积模型：

```py
import tensorflow as tf
import prettytensor as pt
from prettytensor.tutorial import data_utils

tf.app.flags.DEFINE_string('save_path',\
                           None, \
                           'Where to save the model checkpoints.')

FLAGS = tf.app.flags.FLAGS

BATCH_SIZE = 50
EPOCH_SIZE = 60000
TEST_SIZE = 10000
```

由于我们将数据作为 NumPy 数组提供，因此我们需要在图中创建占位符。这些然后必须使用进料`dict`语句馈送：

```py
image_placeholder = tf.placeholder\
                    (tf.float32, [BATCH_SIZE, 28, 28, 1])

labels_placeholder = tf.placeholder\
                     (tf.float32, [BATCH_SIZE, 10])
```

接下来，我们创建`multilayer_fully_connected`函数。前两层是完全连接的（`100`神经元），最后一层是`softmax`结果层。如您所见，链接层是一个非常简单的操作：

```py
def multilayer_fully_connected(images, labels):
  images = pt.wrap(images)
  with pt.defaults_scope\
          (activation_fn=tf.nn.relu,l2loss=0.00001):

    return (images.flatten().\
            fully_connected(100).\
            fully_connected(100).\
            softmax_classifier(10, labels))
```

现在我们将构建一个多层卷积网络：该架构类似于 LeNet 5。请更改此设置，以便您可以尝试其他架构：

```py
def lenet5(images, labels):
  images = pt.wrap(images)
  with pt.defaults_scope\
            (activation_fn=tf.nn.relu, l2loss=0.00001):

    return (images.conv2d(5, 20).\
            max_pool(2, 2).\
            conv2d(5, 50).\
            max_pool(2, 2).\
            flatten().\
            fully_connected(500).\
            softmax_classifier(10, labels))
```

根据所选模型，我们可能有一个 2 层分类器（`multilayer_fully_connected`）或卷积分类器（`lenet5`）：

```py
def make_choice():
    var = int(input('(1) = multy layer model   (2) = lenet 5 '))
    print(var)
    if var == 1:
        result = multilayer_fully_connected\
                 (image_placeholder,labels_placeholder)
        run_model(result)
    elif var == 2:
        result = lenet5\
                 (image_placeholder,labels_placeholder)
        run_model(result) 
    else:
        print ('incorrect input value')
```

最后，我们将为所选模型定义`accuracy`：

```py
def run_model(result):
    accuracy = result.softmax.evaluate_classifier\
                   (labels_placeholder,phase=pt.Phase.test)
```

接下来，我们构建训练和测试集：

```py
  train_images, train_labels = data_utils.mnist(training=True)
  test_images, test_labels = data_utils.mnist(training=False)
```

我们将使用梯度下降优化程序并将其应用于图。 `pt.apply_optimizer`函数增加了正则化损失并设置了一个步进计数器：

```py
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train_op = pt.apply_optimizer\
                   (optimizer,losses=[result.loss])
```

我们可以在正在运行的会话中设置`save_path`，以便每隔一段时间自动保存进度。否则，模型将在会话结束时丢失：

```py
runner = pt.train.Runner(save_path=FLAGS.save_path)
with tf.Session():
    for epoch in range(0,10)
```

随机展示训练数据：

```py
        train_images, train_labels = \
                      data_utils.permute_data\
                      ((train_images, train_labels))

        runner.train_model(train_op,result.\
                           loss,EPOCH_SIZE,\
                           feed_vars=(image_placeholder,\
                                      labels_placeholder),\
                           feed_data=pt.train.\
                           feed_numpy(BATCH_SIZE,\
                                      train_images,\
                                      train_labels),\
                           print_every=100)

        classification_accuracy = runner.evaluate_model\
                                  (accuracy,\
                                   TEST_SIZE,\
                                   feed_vars=(image_placeholder,\
                                              labels_placeholder),\
                                   feed_data=pt.train.\
                                   feed_numpy(BATCH_SIZE,\
                                              test_images,\
                                              test_labels))      
    print("epoch" , epoch + 1)
    print("accuracy", classification_accuracy )

if __name__ == '__main__':
    make_choice()
```

运行示例，我们必须选择要训练的模型：

```py
(1) = multylayer model   (2) = lenet 5
```

通过选择`multylayer model`，我们应该具有 95.5％的准确率：

```py
Extracting /tmp/data\train-images-idx3-ubyte.gz
Extracting /tmp/data\train-labels-idx1-ubyte.gz
Extracting /tmp/data\t10k-images-idx3-ubyte.gz
Extracting /tmp/data\t10k-labels-idx1-ubyte.gz
epoch 1
accuracy [0.8969]
epoch 2
accuracy [0.914]
epoch 3
accuracy [0.9188]
epoch 4
accuracy [0.9306]
epoch 5
accuracy [0.9353]
epoch 6
accuracy [0.9384]
epoch 7
accuracy [0.9445]
epoch 8
accuracy [0.9472]
epoch 9
accuracy [0.9531]
epoch 10
accuracy [0.9552]
```

而对于 Lenet5，我们应该具有 98.8％的准确率：

```py
Extracting /tmp/data\train-images-idx3-ubyte.gz
Extracting /tmp/data\train-labels-idx1-ubyte.gz
Extracting /tmp/data\t10k-images-idx3-ubyte.gz
Extracting /tmp/data\t10k-labels-idx1-ubyte.gz

epoch 1
accuracy [0.9686]
epoch 2
accuracy [0.9755]
epoch 3
accuracy [0.983]
epoch 4
accuracy [0.9841]
epoch 5
accuracy [0.9844]
epoch 6
accuracy [0.9863]
epoch 7
accuracy [0.9862]
epoch 8
accuracy [0.9877]
epoch 9
accuracy [0.9855]
epoch 10
accuracy [0.9886]
```