# OpenAI Gym

OpenAI Gym 是一个开源 Python 框架，由非营利性 AI 研究公司 OpenAI 开发，作为开发和评估 RL 算法的工具包。它给我们提供了一组测试问题，称为环境，我们可以编写 RL 算法来解决。这使我们能够将更多的时间用于实现和改进学习算法，而不是花费大量时间来模拟环境。此外，它为人们提供了一种比较和审查其他算法的媒介。

## OpenAI 环境

OpenAI Gym 拥有一系列环境。在编写本书时，可以使用以下环境：

*   经典控制和玩具文本：来自 RL 文献的小规模任务。
*   算法：执行计算，例如添加多位数和反转序列。这些任务中的大多数都需要记忆，并且可以通过改变序列长度来改变它们的难度。
*   Atari：经典  Atari 游戏，使用街机学习环境，屏幕图像或 RAM 作为输入。
*   棋盘游戏：目前，我们已经将 Go 游戏包括在 9x9 和 19x19 板上，而 Pachi 引擎[13]则作为对手。
*   2D 和 3D 机器人：允许在模拟中控制机器人。这些任务使用 MuJoCo 物理引擎，该引擎专为快速准确的机器人仿真而设计。一些任务改编自 RLLab。

## env 类

OpenAI Gym 允许使用`env`类，它封装了环境和任何内部动态。此类具有不同的方法和属性，使您可以实现创建新环境。最重要的方法名为`reset`，`step`和`render`：

*   `reset`方法的任务是通过将环境初始化为初始状态来重置环境。在重置方法中，必须包含构成环境的元素的定义（在这种情况下，机械臂的定义，要抓取的对象及其支持）。
*   `step`方法是用于在时间上推进环境的  。它需要输入操作并将新观察结果返回给智能体。在该方法中，必须定义运动动态管理，状态和奖励计算以及剧集完成控制。
*   最后一种方法是`render`，  用于显示当前状态。

使用框架提出的`env`类作为新环境的基础，它采用工具包提供的通用接口。

这样，构建的环境可以集成到工具包的库中，并且可以从 OpenAI Gym 社区的用户所做的算法中学习它们的动态。

## 安装并运行 OpenAI Gym

有关如何使用和运行 OpenAI Gym 的更多详细说明，请参阅（[此链接](https://gym.openai.com/docs/)）的官方文档页面。使用以下命令可以实现 OpenApp Gym 的最小安装：

```py
git clone https://github.com/openai/gym
cd gym
pip install -e

```

安装 OpenAI Gym 之后，您可以在 Python 代码中实例化并运行环境：

```py
import gym
env = gym.make('CartPole-v0')

obs = env.reset()

for step_idx in range(500):
  env.render()
  obs, reward, done, _ = env.step(env.action_space.sample())
```

此代码段将首先导入`gym`库。然后它创建了 [Cart-Pole](https://gym.openai.com/envs/CartPole-v0/) 环境的实例  ，这是 RL 中的经典问题。 Cart-Pole 环境模拟安装在推车上的倒立摆。钟摆最初是垂直的，你的目标是保持其垂直平衡。控制摆锤的唯一方法是选择水平方向让推车移动（向左或向右）。

上面的代码运行`500`时间步的环境，并选择随机操作在每一步执行。因此，您在下面的视频中看到，杆不能长时间保持稳定。奖励是通过在杆距垂直方向超过 15 度之前经过的时间步数来衡量的。您保持在此范围内的时间越长，您的总奖励就越高。