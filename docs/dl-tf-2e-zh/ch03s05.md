# 总结

我们已经了解了如何实现 FFNN 架构，其特征在于一组输入单元，一组输出单元以及一个或多个连接该输出的输入级别的隐藏单元。我们已经看到如何组织网络层，以便级别之间的连接是完全的并且在单个方向上：每个单元从前一层的所有单元接收信号并发送其输出值，适当地权衡到所有单元的下一层。

我们还看到了如何为每个层定义激活函数（例如，sigmoid，ReLU，tanh 和 softmax），其中激活函数的选择取决于架构和要解决的问题。

然后，我们实现了四种不同的 FFNN 模型。第一个模型有一个隐藏层，具有 softmax 激活函数。其他三个更复杂的模型总共有五个隐藏层，但具有不同的激活函数。我们还看到了如何使用 TensorFlow 实现深度 MLP 和 DBN，以解决分类任务。使用这些实现，我们设法达到了 90％以上的准确率。最后，我们讨论了如何调整 DNN 的超参数以获得更好和更优化的表现。

虽然常规的 FFNN（例如 MLP）适用于小图像（例如，MNIST 或 CIFAR-10），但由于需要大量参数，它会因较大的图像而分解。例如，100×100 图像具有 10,000 个像素，并且如果第一层仅具有 1,000 个神经元（其已经严格限制传输到下一层的信息量），则这意味着 1000 万个连接。另外，这仅适用于第一层。

重要的是，DNN 不知道像素的组织方式，因此不知道附近的像素是否接近。 CNN 的架构嵌入了这种先验知识。较低层通常识别图像的单元域中的特征，而较高层将较低层特征组合成较大特征。这适用于大多数自然图像，与 DNN 相比，CNN 具有决定性的先机性。

在下一章中，我们将进一步探讨神经网络模型的复杂性，引入 CNN，这可能对深度学习技术产生重大影响。我们将研究主要功能并查看一些实现示例。