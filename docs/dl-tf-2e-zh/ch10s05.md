# 总结

许多研究人员认为，RL 是我们创造人工智能的最好方法。这是一个令人兴奋的领域，有许多未解决的挑战和巨大的潜力。虽然一开始看起来很有挑战性，但开始使用 RL 并不是那么困难。在本章中，我们描述了 RL 的一些基本原理。

我们讨论的主要内容是 Q-Learning 算法。它的显着特点是能够在即时奖励和延迟奖励之间做出选择。最简单的 Q-learning 使用表来存储数据。当监视/控制的系统的状态/动作空间的大小增加时，这很快就失去了可行性。

我们可以使用神经网络作为函数逼近器来克服这个问题，该函数逼近器将状态和动作作为输入并输出相应的 Q 值。

按照这个想法，我们使用 TensorFlow 框架和 OpenAI Gym 工具包实现了一个 Q 学习神经网络，以赢得 FrozenLake 游戏。

在本章的最后一部分，我们介绍了深度强化学习。在传统的 RL 中，问题空间非常有限，并且环境中只有少数可能的状态。这是传统方法的主要局限之一。多年来，已经有一些相对成功的方法能够通过近似状态来处理更大的状态空间。

深度学习算法的进步已经在 RL 中引入了新的成功应用浪潮，因为它提供了有效处理高维输入数据（例如图像）的机会。在这种情况下，训练有素的 DNN 可以被视为一种端到端的 RL 方法，其中智能体可以直接从其输入数据中学习状态抽象和策略近似。按照这种方法，我们实现了 DNN 来解决 Cart-Pole 问题。

我们的 TensorFlow 深度学习之旅将在此结束。深度学习是一个非常有成效的研究领域;有许多书籍，课程和在线资源可以帮助读者深入理论和编程。此外，TensorFlow 还提供了丰富的工具来处理深度学习模型。我希望本书的读者成为 TensorFlow 社区的一部分，该社区非常活跃，并希望热心人士尽快加入。