# 用于情感分析的 LSTM 预测模型

情感分析是 NLP 中使用最广泛的任务之一。 LSTM 网络可用于将短文本分类为期望的类别，即分类问题。例如，一组推文可以分为正面或负面。在本节中，我们将看到这样一个例子。

## 网络设计

实现的 LSTM 网络将具有三层：嵌入层，RNN 层和 softmax 层。从下图可以看到对此的高级视图。在这里，我总结了所有层的功能：

*   嵌入层：我们将在第 8 章中看到一个示例，显示文本数据集不能直接馈送到深度神经网络（DNN），因此一个名为嵌入层是必需的。对于该层，我们将每个输入（k 个单词的张量）变换为 k 个 N 维向量的张量。这称为字嵌入，其中 N 是嵌入大小。每个单词都与在训练过程中需要学习的权重向量相关联。您可以在单词的向量表示中更深入地了解单词嵌入。
*   RNN 层：一旦我们构建了嵌入层，就会有一个名为 RNN 层的新层，它由带有压降包装的 LSTM 单元组成。在训练过程中需要学习 LSTM 权重，如前几节所述。动态展开 RNN 层（如图 4 所示），将 k 个字嵌入作为输入并输出 k 个 M 维向量，其中 M 是 LSTM 单元的隐藏大小。
*   Softmax 或 Sigmoid 层：RNN 层的输出在`k`个时间步长上平均，获得大小为`M`的单个张量。最后，例如，softmax 层用于计算分类概率。

    ![Network design](img/B09698_06_21.jpg)

    图 20：用于情感分析的 LSTM 网络的高级视图

稍后我们将看到交叉熵如何用作损失函数，`RMSProp`是最小化它的优化器。

## LSTM 模型训练

UMICH SI650 - 情感分类数据集（删除了重复）包含有关密歇根大学捐赠的产品和电影评论的数据，可以从[此链接下载](https://inclass.kaggle.com/c/si650winter11/data/)。在获取令牌之前，已经清除了不需要的或特殊的字符（参见`data.csv`文件）。

以下脚本还会删除停用词（请参阅`data_preparation.py`）。给出一些标记为阴性或阳性的样本（1 为阳性，0 为阴性）：

| 情感 | SentimentText |
| --- | --- |
| 1 | 达芬奇密码书真棒。 |
| 1 | 我很喜欢达芬奇密码。 |
| 0 | 天哪，我讨厌断背山。 |
| 0 | 我讨厌哈利波特。 |

> 表 1：情感数据集的样本

现在，让我们看一下为此任务训练 LSTM 网络的分步示例。首先，我们导入必要的模块和包（执行`train.py`文件）：

```py
from data_preparation import Preprocessing
from lstm_network import LSTM_RNN_Network
import tensorflow as tf
import pickle
import datetime
import time
import os
import matplotlib.pyplot as plt
```

在前面的导入声明中，`data_preparation`和`lstm_network`是两个辅助 Python 脚本，用于数据集准备和网络设计。我们稍后会看到更多细节。现在让我们为 LSTM 定义参数：

```py
data_dir = 'data/' # Data directory containing 'data.csv'
stopwords_file = 'data/stopwords.txt' # Path to stopwords file
n_samples= None # Set n_samples=None to use the whole dataset

# Directory where TensorFlow summaries will be stored'
summaries_dir= 'logs/'
batch_size = 100 #Batch size
train_steps = 1000 #Number of training steps
hidden_size= 75 # Hidden size of LSTM layer
embedding_size = 75 # Size of embeddings layer
learning_rate = 0.01
test_size = 0.2
dropout_keep_prob = 0.5 # Dropout keep-probability
sequence_len = None # Maximum sequence length
validate_every = 100 # Step frequency to validate
```

我相信前面的参数是不言自明的。下一个任务是准备 TensorBoard 使用的摘要：

```py
summaries_dir = '{0}/{1}'.format(summaries_dir, datetime.datetime.now().strftime('%d_%b_%Y-%H_%M_%S'))
train_writer = tf.summary.FileWriter(summaries_dir + '/train')
validation_writer = tf.summary.FileWriter(summaries_dir + '/validation')
```

现在让我们准备模型目录：

```py
model_name = str(int(time.time()))
model_dir = '{0}/{1}'.format(checkpoints_root, model_name)
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
```

接下来，让我们准备数据并构建 TensorFlow 图（参见`data_preparation.py`文件）：

```py
data_lstm = Preprocessing(data_dir=data_dir,
                 stopwords_file=stopwords_file,
                 sequence_len=sequence_len,
                 test_size=test_size,
                 val_samples=batch_size,
                 n_samples=n_samples,
                 random_state=100)
```

在前面的代码段中，`Preprocessing`是一个继续的类（详见`data_preparation.py`）几个函数和构造函数，它们帮助我们预处理训练和测试集以训练 LSTM 网络。在这里，我提供了每个函数及其功能的代码。

该类的构造函数初始化数据预处理器。此类提供了一个接口，用于将数据加载，预处理和拆分为训练，验证和测试集。它需要以下参数：

*   `data_dir`：包含数据集文件`data.csv`的数据目录，其中包含名为`SentimentText`和`Sentiment`的列。
*   `stopwords_file`：可选。如果提供，它将丢弃原始数据中的每个停用词。
*   `sequence_len`：可选。如果`m`是数据集中的最大序列长度，则需要`sequence_len &gt;= m`。如果`sequence_len`为`None`，则会自动分配给`m`。
*   `n_samples`：可选。它是从数据集加载的样本数（对大型数据集很有用）。如果`n_samples`是`None`，则将加载整个数据集（注意;如果数据集很大，则可能需要一段时间来预处理每个样本）。
*   `test_size`：可选。 `0&lt;test_size&lt;1`。它表示要包含在测试集中的数据集的比例（默认值为`0.2`）。
*   `val_samples`：可选但可用于表示验证样本的绝对数量（默认为`100`）。
*   `random_state`：这是随机种子的可选参数，用于将数据分成训练，测试和验证集（默认为`0`）。
*   `ensure_preprocessed`：可选。如果`ensure_preprocessed=True`，它确保数据集已经过预处理（默认为`False`）。

构造函数的代码如下：

```py
def __init__(self, data_dir, stopwords_file=None, sequence_len=None, n_samples=None, test_size=0.2, val_samples=100, random_state=0, ensure_preprocessed=False):
        self._stopwords_file = stopwords_file
        self._n_samples = n_samples
        self.sequence_len = sequence_len
        self._input_file = os.path.join(data_dir, 'data.csv')
        self._preprocessed_file=os.path.join(data_dir,"preprocessed_"+str(n_samples)+ ".npz")
        self._vocab_file = os.path.join(data_dir,"vocab_" + str(n_samples) + ".pkl")
        self._tensors = None
        self._sentiments = None
        self._lengths = None
        self._vocab = None
        self.vocab_size = None

        # Prepare data
        if os.path.exists(self._preprocessed_file)and os.path.exists(self._vocab_file):
            print('Loading preprocessed files ...')
            self.__load_preprocessed()
        else:
            if ensure_preprocessed:
                raise ValueError('Unable to findpreprocessed files.')
            print('Reading data ...')
            self.__preprocess()
        # Split data in train, validation and test sets
        indices = np.arange(len(self._sentiments))
        x_tv, self._x_test, y_tv, self._y_test,tv_indices, test_indices = train_test_split(
            self._tensors,
            self._sentiments,
            indices,
            test_size=test_size,
            random_state=random_state,
            stratify=self._sentiments[:, 0])
            self._x_train,self._x_val,self._y_train,self._y_val,train_indices,val_indices= train_test_split(x_tv, y_tv, tv_indices, test_size=val_samples,random_state = random_state,
               stratify=y_tv[:, 0])
        self._val_indices = val_indices
        self._test_indices = test_indices
        self._train_lengths = self._lengths[train_indices]
        self._val_lengths = self._lengths[val_indices]
        self._test_lengths = self._lengths[test_indices]
        self._current_index = 0
        self._epoch_completed = 0 
```

现在让我们看看前面方法的签名。我们从`_preprocess()`方法开始，该方法从`data_dir` / `data.csv`加载数据，预处理每个加载的样本，并存储中间文件以避免以后进行预处理。工作流程如下：

1.  加载数据
2.  清理示例文本
3.  准备词汇词典
4.  删除最不常见的单词（它们可能是语法错误），将样本编码为张量，并根据`sequence_len`用零填充每个张量
5.  保存中间文件
6.  存储样本长度以备将来使用

现在让我们看看下面的代码块，它代表了前面的工作流程：

```py
def __preprocess(self):
    data = pd.read_csv(self._input_file, nrows=self._n_samples)
    self._sentiments = np.squeeze(data.as_matrix(columns=['Sentiment']))
    self._sentiments = np.eye(2)[self._sentiments]
    samples = data.as_matrix(columns=['SentimentText'])[:, 0]
    samples = self.__clean_samples(samples)
    vocab = dict()
    vocab[''] = (0, len(samples))  # add empty word
    for sample in samples:
        sample_words = sample.split()
        for word in list(set(sample_words)):  # distinct words
            value = vocab.get(word)
            if value is None:
                vocab[word] = (-1, 1)
            else:
                encoding, count = value
                vocab[word] = (-1, count + 1)
      sample_lengths = []
      tensors = []
      word_count = 1
      for sample in samples:
          sample_words = sample.split()
          encoded_sample = []
          for word in list(set(sample_words)):  # distinct words 
              value = vocab.get(word)
              if value is not None:
                  encoding, count = value
                  if count / len(samples) > 0.0001:
                      if encoding == -1:
                          encoding = word_count
                          vocab[word] = (encoding, count)
                          word_count += 1
                      encoded_sample += [encoding]
                  else:
                      del vocab[word]
          tensors += [encoded_sample]
          sample_lengths += [len(encoded_sample)]
      self.vocab_size = len(vocab)
      self._vocab = vocab
      self._lengths = np.array(sample_lengths)
      self.sequence_len, self._tensors = self.__apply_to_zeros(tensors, self.sequence_len)
      with open(self._vocab_file, 'wb') as f:
          pickle.dump(self._vocab, f)
      np.savez(self._preprocessed_file, tensors=self._tensors, lengths=self._lengths, sentiments=self._sentiments)
```

接下来，我们调用前面的方法并加载中间文件，避免数据预处理：

```py
def __load_preprocessed(self):
    with open(self._vocab_file, 'rb') as f:
        self._vocab = pickle.load(f)
    self.vocab_size = len(self._vocab)
    load_dict = np.load(self._preprocessed_file)
    self._lengths = load_dict['lengths']
    self._tensors = load_dict['tensors']
    self._sentiments = load_dict['sentiments']
    self.sequence_len = len(self._tensors[0])
```

一旦我们预处理数据集，下一个任务就是清理样本。工作流程如下：

1.  准备正则表达式模式。
2.  清洁每个样本。
3.  恢复 HTML 字符。
4.  删除@users 和 URL。
5.  转换为小写。
6.  删除标点符号。
7.  用 C 替换 CC（C +）（连续出现两次以上的字符）
8.  删除停用词。

现在让我们以编程方式编写上述步骤。为此，我们有以下函数：

```py
def __clean_samples(self, samples):
    print('Cleaning samples ...')
    ret = []
    reg_punct = '[' + re.escape(''.join(string.punctuation)) + ']'
    if self._stopwords_file is not None:
        stopwords = self.__read_stopwords()
        sw_pattern = re.compile(r'\b(' + '|'.join(stopwords) + r')\b')
    for sample in samples:
        text = html.unescape(sample)
        words = text.split()
        words = [word for word in words if not word.startswith('@') and not word.startswith('http://')]
        text = ' '.join(words)
        text = text.lower()
        text = re.sub(reg_punct, ' ', text)
        text = re.sub(r'([a-z])\1{2,}', r'\1', text)
        if stopwords is not None:
            text = sw_pattern.sub('', text)
        ret += [text]
    return ret
```

`__apply_to_zeros()`方法返回使用的`padding_length`和填充张量的 NumPy 数组。首先，它找到最大长度`m`，并确保`m&gt;=sequence_len`。然后根据`sequence_len`用零填充列表：

```py
def __apply_to_zeros(self, lst, sequence_len=None):
    inner_max_len = max(map(len, lst))
    if sequence_len is not None:
        if inner_max_len > sequence_len:
            raise Exception('Error: Provided sequence length is not sufficient')
        else:
            inner_max_len = sequence_len
result = np.zeros([len(lst), inner_max_len], np.int32)
for i, row in enumerate(lst):
    for j, val in enumerate(row):
        result[i][j] = val
return inner_max_len, result
```

下一个任务是删除所有停用词（在`data` / `StopWords.txt file`中提供）。此方法返回停用词列表：

```py
def __read_stopwords(self):
    if self._stopwords_file is None:
        return None
    with open(self._stopwords_file, mode='r') as f:
        stopwords = f.read().splitlines()
    return stopwords
```

`next_batch()`方法将`batch_size&gt;0`作为包含的样本数，在完成周期后返回批量大小样本（`text_tensor`，`text_target`，`text_length`），并随机抽取训练样本：

```py
def next_batch(self, batch_size):
    start = self._current_index
    self._current_index += batch_size
    if self._current_index > len(self._y_train):
        self._epoch_completed += 1
        ind = np.arange(len(self._y_train))
        np.random.shuffle(ind)
        self._x_train = self._x_train[ind]
        self._y_train = self._y_train[ind]
        self._train_lengths = self._train_lengths[ind]
        start = 0
        self._current_index = batch_size
    end = self._current_index
    return self._x_train[start:end], self._y_train[start:end], self._train_lengths[start:end]
```

然后使用称为`get_val_data()`的下一个方法来获取在训练期间使用的验证集。它接受原始文本并返回验证数据。默认情况下，它返回`original_text`（`original_samples`，`text_tensor`，`text_target`，`text_length`），否则返回`text_tensor`，`text_target`，`text_length`：

```py
def get_val_data(self, original_text=False):
    if original_text:
        data = pd.read_csv(self._input_file, nrows=self._n_samples)
        samples = data.as_matrix(columns=['SentimentText'])[:, 0]
        return samples[self._val_indices], self._x_val, self._y_val, self._val_lengths
        return self._x_val, self._y_val, self._val_lengths
```

最后，  是一个名为`get_test_data()`的附加方法，用于准备将在模型评估期间使用的测试集：

```py
    def get_test_data(self, original_text=False):
        if original_text:
            data = pd.read_csv(self._input_file, nrows=self._n_samples)
            samples = data.as_matrix(columns=['SentimentText'])[:, 0]
            return samples[self._test_indices], self._x_test, self._y_test, self._test_lengths
        return self._x_test, self._y_test, self._test_lengths
```

现在我们准备数据，以便 LSTM 网络可以提供它：

```py
lstm_model = LSTM_RNN_Network(hidden_size=[hidden_size],
                              vocab_size=data_lstm.vocab_size,
                              embedding_size=embedding_size,
                              max_length=data_lstm.sequence_len,
                              learning_rate=learning_rate)
```

在前面的代码段中，`LSTM_RNN_Network`是一个包含多个函数和构造函数的类，可帮助我们创建 LSTM 网络。即将推出的构造函数构建了 TensorFlow LSTM 模型。它需要以下参数：

*   `hidden_size`：一个数组，保存 rnn 层的 LSTM 单元中的单元数
*   `vocab_size`：样本中的词汇量大小
*   `embedding_size`：将使用此大小的向量对单词进行编码
*   `max_length`：输入张量的最大长度
*   `n_classes`：分类类的数量
*   `learning_rate`：RMSProp 算法的学习率
*   `random_state`：dropout 的随机状态

构造函数的代码如下：

```py
def __init__(self, hidden_size, vocab_size, embedding_size, max_length, n_classes=2, learning_rate=0.01, random_state=None):
    # Build TensorFlow graph
    self.input = self.__input(max_length)
    self.seq_len = self.__seq_len()
    self.target = self.__target(n_classes)
    self.dropout_keep_prob = self.__dropout_keep_prob()
    self.word_embeddings = self.__word_embeddings(self.input, vocab_size, embedding_size, random_state)
    self.scores = self.__scores(self.word_embeddings, self.seq_len, hidden_size, n_classes, self.dropout_keep_prob,
                                random_state)
        self.predict = self.__predict(self.scores)
        self.losses = self.__losses(self.scores, self.target)
        self.loss = self.__loss(self.losses)
        self.train_step = self.__train_step(learning_rate, self.loss)
        self.accuracy = self.__accuracy(self.predict, self.target)
        self.merged = tf.summary.merge_all()
```

下一个函数被称为`_input()`，它采用一个名为 param `max_length`的参数，它是输入张量的最大长度。然后它返回一个输入占位符，其形状为`[batch_size, max_length]`，用于 TensorFlow 计算：

```py
    def __input(self, max_length):
        return tf.placeholder(tf.int32, [None, max_length], name='input')
```

接下来，`_seq_len()`函数返回一个形状为`[batch_size]`的序列长度占位符。它保持给定批次中每个张量的实际长度，允许动态序列长度：

```py
def __seq_len(self):
    return tf.placeholder(tf.int32, [None], name='lengths')
```

下一个函数称为`_target()`。它需要一个名为 param `n_classes`的参数，它包含分类类的数量。最后，它返回形状为`[batch_size, n_classes]`的目标占位符：

```py
def __target(self, n_classes):
    return tf.placeholder(tf.float32, [None, n_classes], name='target')
```

`_dropout_keep_prob()`返回一个持有 dropout 的占位符保持概率以减少过拟合：

```py
def __dropout_keep_prob(self):
    return tf.placeholder(tf.float32, name='dropout_keep_prob')
```

`_cell()`方法用于构建带有压差包装器的 LSTM 单元。它需要以下参数：

*   `hidden_size`：它是 LSTM 单元中的单元数
*   `dropout_keep_prob`：这表示持有 dropout 保持概率的张量
*   `seed`：它是一个可选值，可确保 dropout 包装器的随机状态计算的可重现性。

最后，它返回一个带有 dropout 包装器的 LSTM 单元：

```py
def __cell(self, hidden_size, dropout_keep_prob, seed=None):
    lstm_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)
    dropout_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=dropout_keep_prob, output_keep_prob = dropout_keep_prob, seed=seed)
        return dropout_cell
```

一旦我们创建了 LSTM 单元格，我们就可以创建输入标记的嵌入。为此，`__word_embeddings()`可以解决这个问题。它构建一个形状为`[vocab_size, embedding_size]`的嵌入层，输入参数如`x`，它是形状`[batch_size, max_length]`的输入。 `vocab_size`是词汇量大小，即可能出现在样本中的可能单词的数量。 `embedding_size`是将使用此大小的向量表示的单词，种子是可选的，但确保嵌入初始化的随机状态。

最后，它返回具有形状`[batch_size, max_length, embedding_size]`的嵌入查找张量：

```py
def __word_embeddings(self, x, vocab_size, embedding_size, seed=None):
    with tf.name_scope('word_embeddings'):
        embeddings = tf.get_variable("embeddings",shape=[vocab_size, embedding_size], dtype=tf.float32, initializer=None, regularizer=None, trainable=True, collections=None)
        embedded_words = tf.nn.embedding_lookup(embeddings, x)
    return embedded_words
```

`__rnn_layer ()`方法创建 LSTM 层。它需要几个输入参数，这里描述：

*   `hidden_size`：这是 LSTM 单元中的单元数
*   `x`：这是带形状的输入
*   `seq_len`：这是具有形状的序列长度张量
*   `dropout_keep_prob`：这是持有 dropout 保持概率的张量
*   `variable_scope`：这是变量范围的名称（默认层是`rnn_layer`）
*   `random_state`：这是 dropout 包装器的随机状态

最后，它返回形状为`[batch_size, max_seq_len, hidden_size]`的输出：

```py
def __rnn_layer(self, hidden_size, x, seq_len, dropout_keep_prob, variable_scope=None, random_state=None):
    with tf.variable_scope(variable_scope, default_name='rnn_layer'):
        lstm_cell = self.__cell(hidden_size, dropout_keep_prob, random_state)
        outputs, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seq_len)
    return outputs
```

`_score()`方法用于计算网络输出。它需要几个输入参数，如下所示：

*   `embedded_words`：这是具有形状`[batch_size, max_length, embedding_size]`的嵌入查找张量
*   `seq_len`：这是形状`[batch_size]`的序列长度张量
*   `hidden_size`：这是一个数组，其中包含每个 RNN 层中 LSTM 单元中的单元数
*   `n_classes`：这是分类的数量
*   `dropout_keep_prob`：这是持有 dropout 保持概率的张量
*   `random_state`：这是一个可选参数，但它可用于确保 dropout 包装器的随机状态

最后，`_score()`方法返回具有形状`[batch_size, n_classes]`的每个类的线性激活：

```py
def __scores(self, embedded_words, seq_len, hidden_size, n_classes, dropout_keep_prob, random_state=None):
    outputs = embedded_words
    for h in hidden_size:
        outputs = self.__rnn_layer(h, outputs, seq_len, dropout_keep_prob)
    outputs = tf.reduce_mean(outputs, axis=[1])
    with tf.name_scope('final_layer/weights'):
        w = tf.get_variable("w", shape=[hidden_size[-1], n_classes], dtype=tf.float32, initializer=None, regularizer=None, trainable=True, collections=None)
        self.variable_summaries(w, 'final_layer/weights')
    with tf.name_scope('final_layer/biases'):
        b = tf.get_variable("b", shape=[n_classes], dtype=tf.float32, initializer=None, regularizer=None,trainable=True, collections=None)
        self.variable_summaries(b, 'final_layer/biases')
        with tf.name_scope('final_layer/wx_plus_b'):
            scores = tf.nn.xw_plus_b(outputs, w, b, name='scores')
            tf.summary.histogram('final_layer/wx_plus_b', scores)
        return scores
```

`_predict()`方法将得分作为具有形状`[batch_size, n_classes]`的每个类的线性激活，并以形状`[batch_size, n_classes]`返回 softmax（以`[0, 1]`的比例标准化得分）激活：

```py
def __predict(self, scores):
    with tf.name_scope('final_layer/softmax'):
        softmax = tf.nn.softmax(scores, name='predictions')
        tf.summary.histogram('final_layer/softmax', softmax)
    return softmax
```

`_losses()`方法返回具有形状`[batch_size]`的交叉熵损失（因为 softmax 用作激活函数）。它还需要两个参数，例如得分，作为具有形状`[batch_size, n_classes]`的每个类的线性激活和具有形状`[batch_size, n_classes]`的目标张量：

```py
def __losses(self, scores, target):
        with tf.name_scope('cross_entropy'):
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=scores, labels=target, name='cross_entropy')
        return cross_entropy
```

`_loss()`函数计算并返回平均交叉熵损失。它只需要一个参数，称为损耗，它表示形状`[batch_size]`的交叉熵损失，并由前一个函数计算：

```py
def __loss(self, losses):
    with tf.name_scope('loss'):
        loss = tf.reduce_mean(losses, name='loss')
        tf.summary.scalar('loss', loss)
    return loss
```

现在，`_train_step()`计算并返回`RMSProp`训练步骤操作。它需要两个参数，`learning_rate`，这是`RMSProp`优化器的学习率;和前一个函数计算的平均交叉熵损失：

```py
def __train_step(self, learning_rate, loss):
    return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)
```

评估表现时，`_accuracy()`函数计算分类的准确率。它需要三个参数，预测，softmax 激活具有哪种形状`[batch_size, n_classes]`;和具有形状`[batch_size, n_classes]`的目标张量和当前批次中获得的平均精度：

```py
def __accuracy(self, predict, target):
    with tf.name_scope('accuracy'):
        correct_pred = tf.equal(tf.argmax(predict, 1), tf.argmax(target, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')
        tf.summary.scalar('accuracy', accuracy)
    return accuracy
```

下一个函数被称为`initialize_all_variable()`，正如您可能猜到的那样，它初始化所有变量：

```py
def initialize_all_variables(self):
    return tf.global_variables_initializer()
```

最后，我们有一个名为`variable_summaries()`的静态方法，它将大量摘要附加到 TensorBoard 可视化的张量上。它需要以下参数：

```py
var: is the variable to summarize
mean: mean of the summary name.
```

签名如下：

```py
    @staticmethod
    def variable_summaries(var, name):
        with tf.name_scope('summaries'):
            mean = tf.reduce_mean(var)
            tf.summary.scalar('mean/' + name, mean)
            with tf.name_scope('stddev'):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
            tf.summary.scalar('stddev/' + name, stddev)
            tf.summary.scalar('max/' + name, tf.reduce_max(var))
            tf.summary.scalar('min/' + name, tf.reduce_min(var))
            tf.summary.histogram(name, var)
```

现在我们需要在训练模型之前创建一个 TensorFlow 会话：

```py
sess = tf.Session()
```

让我们初始化所有变量：

```py
init_op = tf.global_variables_initializer()
sess.run(init_op)
```

然后我们保存  TensorFlow 模型以备将来使用：

```py
saver = tf.train.Saver()
```

现在让我们准备训练集：

```py
x_val, y_val, val_seq_len = data_lstm.get_val_data()
```

现在我们应该编写 TensorFlow 图计算的日志：

```py
train_writer.add_graph(lstm_model.input.graph)
```

此外，我们可以创建一些空列表来保存训练损失，验证损失和步骤，以便我们以图形方式查看它们：

```py
train_loss_list = []
val_loss_list = []
step_list = []
sub_step_list = []
step = 0
```

现在我们开始训练。在每个步骤中，我们记录训练错误。验证错误记录在每个子步骤中：

```py
for i in range(train_steps):
    x_train, y_train, train_seq_len = data_lstm.next_batch(batch_size)
    train_loss, _, summary = sess.run([lstm_model.loss, lstm_model.train_step, lstm_model.merged],
                                      feed_dict={lstm_model.input: x_train,
                                                 lstm_model.target: y_train,
                                                 lstm_model.seq_len: train_seq_len,
                                                 lstm_model.dropout_keep_prob:dropout_keep_prob})
    train_writer.add_summary(summary, i)  # Write train summary for step i (TensorBoard visualization)
    train_loss_list.append(train_loss)
    step_list.append(i)
        print('{0}/{1} train loss: {2:.4f}'.format(i + 1, FLAGS.train_steps, train_loss))
    if (i + 1) %validate_every == 0:
        val_loss, accuracy, summary = sess.run([lstm_model.loss, lstm_model.accuracy, lstm_model.merged],
                                               feed_dict={lstm_model.input: x_val,
                                                          lstm_model.target: y_val,
                                                          lstm_model.seq_len: val_seq_len,
                                                          lstm_model.dropout_keep_prob: 1})
        validation_writer.add_summary(summary, i)  
        print('   validation loss: {0:.4f} (accuracy {1:.4f})'.format(val_loss, accuracy))
        step = step + 1
        val_loss_list.append(val_loss)
        sub_step_list.append(step)
```

以下是上述代码的输出：

```py
>>>

1/1000 train loss: 0.6883
2/1000 train loss: 0.6879
3/1000 train loss: 0.6943

99/1000 train loss: 0.4870
100/1000 train loss: 0.5307
validation loss: 0.4018 (accuracy 0.9200)
…
199/1000 train loss: 0.1103
200/1000 train loss: 0.1032
validation loss: 0.0607 (accuracy 0.9800)
…
299/1000 train loss: 0.0292
300/1000 train loss: 0.0266
validation loss: 0.0417 (accuracy 0.9800)
…
998/1000 train loss: 0.0021
999/1000 train loss: 0.0007
1000/1000 train loss: 0.0004
validation loss: 0.0939 (accuracy 0.9700)

```

上述代码打印了训练和验证错误。训练结束后，模型将保存到具有唯一 ID 的检查点目录中：

```py
checkpoint_file = '{}/model.ckpt'.format(model_dir)
save_path = saver.save(sess, checkpoint_file)
print('Model saved in: {0}'.format(model_dir))
```

以下是上述代码的输出：

```py
>>>
Model saved in checkpoints/1517781236

```

检查点目录将至少生成三个文件：

*   `config.pkl`包含用于训练模型的参数。
*   `model.ckpt`包含模型的权重。
*   `model.ckpt.meta`包含 TensorFlow 图定义。

让我们看看训练是如何进行的，也就是说，训练和验证损失如下：

```py
# Plot loss over time
plt.plot(step_list, train_loss_list, 'r--', label='LSTM training loss per iteration', linewidth=4)
plt.title('LSTM training loss per iteration')
plt.xlabel('Iteration')
plt.ylabel('Training loss')
plt.legend(loc='upper right')
plt.show()

# Plot accuracy over time
plt.plot(sub_step_list, val_loss_list, 'r--', label='LSTM validation loss per validating interval', linewidth=4)
plt.title('LSTM validation loss per validation interval')
plt.xlabel('Validation interval')
plt.ylabel('Validation loss')
plt.legend(loc='upper left')
plt.show()
```

以下是上述代码的输出：

```py
>>>

```

![LSTM model training](img/B09698_06_22.jpg)

图 21：a）测试集上每次迭代的 LSTM 训练损失，b）每个验证间隔的 LSTM 验证损失

如果我们检查前面的绘图，很明显训练阶段和验证阶段的训练都很顺利，只有 1000 步。然而，读者应加大训练步骤，调整超参数，看看它是如何去。

## 通过 TensorBoard 进行可视化

现在让我们观察 TensorBoard 上的 TensorFlow 计算图。只需执行以下命令并在`localhost:6006/`访问 TensorBoard：

```py
tensorboard --logdir /home/logs/
```

图选项卡显示执行图，包括使用的梯度，`loss_op`，精度，最终层，使用的优化器（在我们的例子中是`RMSPro`），LSTM 层（即 RNN 层），嵌入层和`save_op`：

![Visualizing through TensorBoard](img/B09698_06_23.jpg)

图 22：TensorBoard 上的执行图

执行图显示，我们为这种基于 LSTM 的分类器进行的情感分析计算是非常透明的。我们还可以观察层中的验证，训练损失，准确率和操作：

![Visualizing through TensorBoard](img/B09698_06_24.jpg)

图 23：TensorBoard 层中的验证，训练损失，准确率和操作

## LSTM 模型评估

我们已经训练了并保存了我们的 LSTM 模型。我们可以轻松恢复训练模型并进行一些评估。我们需要准备测试集并使用先前训练的 TensorFlow 模型对其进行预测。我们马上做吧。首先，我们加载所需的模型：

```py
import tensorflow as tf
from data_preparation import Preprocessing
   import pickle
Then we load to show the checkpoint directory where the model was saved. For our case, it was checkpoints/1505148083.
```

### 注意

对于此步骤，使用以下命令执行`predict.py`脚本：

```py
$ python3 predict.py --checkpoints_dir checkpoints/1517781236

```

```py
# Change this path based on output by 'python3 train.py' 
checkpoints_dir = 'checkpoints/1517781236' 

ifcheckpoints_dir is None:
    raise ValueError('Please, a valid checkpoints directory is required (--checkpoints_dir <file name>)')
```

现在加载测试数据集并准备它以评估模型：

```py
data_lstm = Preprocessing(data_dir=data_dir,
                 stopwords_file=stopwords_file,
                 sequence_len=sequence_len,
                 n_samples=n_samples,
                 test_size=test_size,
                 val_samples=batch_size,
                 random_state=random_state,
                 ensure_preprocessed=True)
```

在上面的代码中，完全按照我们在训练步骤中的操作使用以下参数：

```py
data_dir = 'data/' # Data directory containing 'data.csv'
stopwords_file = 'data/stopwords.txt' # Path to stopwords file.
sequence_len = None # Maximum sequence length
n_samples= None # Set n_samples=None to use the whole dataset
test_size = 0.2
batch_size = 100 #Batch size
random_state = 0 # Random state used for data splitting. Default is 0
```

此评估方法的工作流程如下：

1.  首先，导入元图并使用测试数据评估模型
2.  为计算创建 TensorFlow 会话
3.  导入图并恢复其权重
4.  恢复输入/输出张量
5.  执行预测
6.  最后，我们在简单的测试集上打印精度和结果

步骤 1 之前已经完成  。此代码执行步骤 2 到 5：

```py
original_text, x_test, y_test, test_seq_len = data_lstm.get_test_data(original_text=True)
graph = tf.Graph()
with graph.as_default():
    sess = tf.Session()    
    print('Restoring graph ...')
    saver = tf.train.import_meta_graph("{}/model.ckpt.meta".format(FLAGS.checkpoints_dir))
    saver.restore(sess, ("{}/model.ckpt".format(checkpoints_dir)))
    input = graph.get_operation_by_name('input').outputs[0]
    target = graph.get_operation_by_name('target').outputs[0]
    seq_len = graph.get_operation_by_name('lengths').outputs[0]
    dropout_keep_prob = graph.get_operation_by_name('dropout_keep_prob').outputs[0]
    predict = graph.get_operation_by_name('final_layer/softmax/predictions').outputs[0]
    accuracy = graph.get_operation_by_name('accuracy/accuracy').outputs[0]
    pred, acc = sess.run([predict, accuracy],
                         feed_dict={input: x_test,
                                    target: y_test,
                                    seq_len: test_seq_len,
                                    dropout_keep_prob: 1})
    print("Evaluation done.")
```

以下是上述代码的输出：

```py
>>>
Restoring graph ...
The evaluation was done.

```

做得好！训练结束了，让我们打印结果：

```py
print('\nAccuracy: {0:.4f}\n'.format(acc))
for i in range(100):
    print('Sample: {0}'.format(original_text[i]))
    print('Predicted sentiment: [{0:.4f}, {1:.4f}]'.format(pred[i, 0], pred[i, 1]))
    print('Real sentiment: {0}\n'.format(y_test[i]))
```

以下是上述代码的输出：

```py
>>>
Accuracy: 0.9858

Sample: I loved the Da Vinci code, but it raises many theological questions most of which are very absurd...
Predicted sentiment: [0.0000, 1.0000]
Real sentiment: [0\. 1.]
…
Sample: I'm sorry I hate to read Harry Potter, but I love the movies!
Predicted sentiment: [1.0000, 0.0000]
Real sentiment: [1\. 0.]
…
Sample: I LOVE Brokeback Mountain...
Predicted sentiment: [0.0002, 0.9998]
Real sentiment: [0\. 1.]
…
Sample: We also went to see Brokeback Mountain which totally SUCKED!!!
Predicted sentiment: [1.0000, 0.0000]
Real sentiment: [1\. 0.]

```

精度高于 98％。这太棒了！但是，您可以尝试使用调整的超参数迭代训练以获得更高的迭代次数，您可能会获得更高的准确率。我把它留给读者。

在下一节中，我们将看到如何使用 LSTM 开发更高级的 ML 项目，这被称为使用智能手机数据集的人类活动识别。简而言之，我们的 ML 模型将能够将人类运动分为六类：走路，走楼上，走楼下，坐，站立和铺设。