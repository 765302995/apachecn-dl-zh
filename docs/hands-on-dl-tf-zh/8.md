# 单隐藏层模型

在这里，我们将实践神经网络的基础知识。 我们将逻辑回归 TenserFlow 代码改编为神经元的单个隐藏层。 然后，您将学习反向传播背后的思想以计算权重，即训练网络。 最后，您将在 TensorFlow 中训练您的第一个真正的神经网络。

本部分的 TensorFlow 代码应该看起来很熟悉。 它只是逻辑回归代码的略微演变版本。 让我们看看如何添加神经元的隐藏层，以计算输入像素的非线性组合。

您应该从全新的 Python 会话开始，执行代码以读入，并按照逻辑模型中的步骤设置数据。 相同的代码，只是复制到新文件中：

```py
import tensorflow as tf
import numpy as np
import math
from tqdm import tqdm
%autoindent
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, *args, **kwargs):
        return x
```

您总是可以回到前面的部分，并提醒自己该代码的作用； 直到`num_hidden`变量的所有内容都可以使您快速入门。

## 探索单隐藏层模型

现在，让我们逐步介绍单个隐藏层模型：

1.  首先，让我们指定`num_hidden = 128`想要多少个神经元； 最终，这实际上是将多少个非线性组合传递给逻辑对数。
2.  为了适应这一点，我们还需要更新`W1`和`b1`权重张量的形状。 他们现在正在馈送我们隐藏的神经元，因此需要匹配形状：

    ```py
    W1 = tf.Variable(tf.truncated_normal([1296, num_hidden],
                                       stddev=1./math.sqrt(1296)))
    b1 = tf.Variable(tf.constant(0.1,shape=[num_hidden]))
    ```

3.  The way we compute the activation function of the weighted sum is with the single `h1` line; this is to multiply our input pixels by their respective weights for each neuron:

    ```py
    h1 = tf.sigmoid(tf.matmul(x,W1) + b1)
    ```

    添加神经元偏差项，最后通过`sigmoid`激活函数进行设置； 此时，我们有 128 个中间值：

    ![Exploring the single hidden layer model](img/00028.jpg)

4.  Now it's just your friendly logistic regression again; you already know what to do. These newly computed 128 features need their own set of weights and biases to compute a score on the output class, that's `W2` and `b2`, respectively. Note how the shape matches the shape of the neurons 128, and the number of the output class is `5`:

    ```py
    W2 = tf.Variable(tf.truncated_normal([num_hidden, 5],
                                          stddev=1./math.sqrt(5)))
    b2 = tf.Variable(tf.constant(0.1,shape=[5]))
    sess.run(tf.global_variables_initializer())
    ```

    在所有这些权重中，我们使用此奇怪的截断普通调用对其进行初始化。 借助神经网络，我们希望获得良好的初始值分布，以便我们的权重可以攀升至有意义的值，而不是仅仅归零。

5.  截断正态具有给定标准偏差的正态分布中的随机值，该研究标准按输入数量进行缩放，但抛出的值太极端，因此被截断了。 定义好权重和神经元后，我们将像以前一样设置最终的`softmax`模型，除了需要注意使用 128 个神经元作为输入`h1`以及相关的权重和偏差`W2` ]和`b2`：

    ```py
    y = tf.nn.softmax(tf.matmul(h1,W2) + b2)
    ```

## 反向传播

训练神经网络和许多其他机器学习模型权重的关键称为反向传播。

![Backpropagation](img/00029.jpg)

完整的推导超出了本书的范围，但是让我们直观地进行研究。 当您在空中训练逻辑回归之类的模型并且训练集直接来自选择不当的权重时，您可以看到应该调整哪些权重以及应该调整多少权重并相应地更改它们。

从形式上讲，TensorFlow 通过计算空气相对于权重的导数并将权重调整为该数值的一小部分来实现此目的。 反向传播实际上是同一过程的扩展。

您从最底层的输出或成本函数层开始，计算导数，然后使用它们来计算与上一层神经元相关的导数。 通过将从成本到权重的路径上的导数乘积相加，我们可以计算相对于要调整的权重的成本的适当偏导数。 上图中显示的公式仅说明了红色箭头显示的内容。 如果这看起来很复杂，请不要担心。

TensorFlow 使用优化器在后台为您处理。 由于我们使用 TensorFlow 精心指定了模型来训练模型，因此几乎与之前完全相同，因此我们将在此处使用相同的代码：

```py
epochs = 5000
train_acc = np.zeros(epochs//10)
test_acc = np.zeros(epochs//10)
for i in tqdm(range(epochs), ascii=True):
    if i % 10 == 0: # Record summary data, and the accuracy
        # Check accuracy on train set
        A = accuracy.eval(feed_dict={x: train.reshape([-1,1296]), y_: onehot_train})
        train_acc[i//10] = A

        # And now the validation set
        A = accuracy.eval(feed_dict={x: test.reshape([-1,1296]), y_: onehot_test})
        test_acc[i//10] = A
    train_step.run(feed_dict={x: train.reshape([-1,1296]), y_: onehot_train})
```

需要注意的一件事是，因为我们有这些隐藏的神经元，所以有更多的权重可以拟合模型。 这意味着我们的模型将需要更长的运行时间，并且必须花费更多的迭代时间才能进行训练。 这次我们通过`5000`历时运行它：

![Backpropagation](img/00030.jpg)

该模型可能比以前的模型花费更长的时间，可能是前一个模型的四倍。 因此，您可能需要几分钟到 10 分钟的时间，具体取决于您的计算机。 现在，通过模型训练，我们将在稍后查看验证准确率。