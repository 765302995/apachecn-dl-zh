# 使用数据源

对于本书的大部分内容，我们将依赖数据集的使用来适应机器学习算法。本节介绍如何通过 TensorFlow 和 Python 访问每个数据集。

> 一些数据源依赖于外部网站的维护，以便您可以访问数据。如果这些网站更改或删除此数据，则可能需要更新本节中的以下某些代码。您可以在作者的[ GitHub 页面](https://github.com/nfmcclure/tensorflow_cookbook)上找到更新的代码。

## 做好准备

在 TensorFlow 中，我们将使用的一些数据集构建在 Python 库中，一些将需要 Python 脚本下载，一些将通过 Internet 手动下载。几乎所有这些数据集都需要有效的 Internet 连接，以便您可以检索它们。

## 操作步骤

1.  虹膜数据：该数据集可以说是机器学习中使用的最经典的数据集，也可能是所有统计数据。它是一个数据集，可以测量三种不同类型鸢尾花的萼片长度，萼片宽度，花瓣长度和花瓣宽度：Iris setosa，Iris virginica 和 Iris versicolor。总共有 150 个测量值，这意味着每个物种有 50 个测量值。要在 Python 中加载数据集，我们将使用 scikit-learn 的数据集函数，如下所示：

```py
from sklearn import datasets 
iris = datasets.load_iris() 
print(len(iris.data)) 
150 
print(len(iris.target)) 
150 
print(iris.data[0]) # Sepal length, Sepal width, Petal length, Petal width 
[ 5.1 3.5 1.4 0.2] 
print(set(iris.target)) # I. setosa, I. virginica, I. versicolor 
{0, 1, 2} 
```

1.  出生体重数据：该数据最初来自 Baystate Medical Center，Springfield，Mass 1986（1）。该数据集包含出生体重的测量以及母亲和家族病史的其他人口统计学和医学测量。有 11 个变量的 189 个观测值。以下代码显示了如何在 Python 中访问此数据：

```py
import requests
birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat' 
birth_file = requests.get(birthdata_url) 
birth_data = birth_file.text.split('\r\n') 
birth_header = birth_data[0].split('\t') 
birth_data = [[float(x) for x in y.split('\t') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]
print(len(birth_data)) 
189 
print(len(birth_data[0])) 
9
```

1.  波士顿住房数据：卡内基梅隆大学在其 StatLib 库中维护着一个数据集库。这些数据可通过[加州大学欧文分校的机器学习库](https://archive.ics.uci.edu/ml/index.php)轻松访问。有 506 个房屋价值观察，以及各种人口统计数据和住房属性（14 个变量）。以下代码显示了如何通过 Keras 库在 Python 中访问此数据：

```py
from keras.datasets import boston_housing
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()
housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] 
print(x_train.shape[0]) 
404 
print(x_train.shape[1]) 
13 
```

1.  MNIST 手写数据：MNIST（混合国家标准与技术研究院）数据集是较大的 NIST 手写数据库的子集。 MNIST 手写数据集托管在[ Yann LeCun 的网站](https://yann.lecun.com/exdb/mnist/)上。它是一个包含 70,000 个单元数字图像（0-9）的数据库，其中约 60,000 个用于训练集注释，10,000 个用于测试集。 TensorFlow 在图像识别中经常使用此数据集，TensorFlow 提供了访问此数据的内置函数。在机器学习中，提供验证数据以防止过拟合（目标泄漏）也很重要。因此，TensorFlow 将 5000 列训练图像留在验证集中。以下代码显示了如何在 Python 中访问此数据：

```py
from tensorflow.examples.tutorials.mnist import input_data 
mnist = input_data.read_data_sets("MNIST_data/"," one_hot=True) 
print(len(mnist.train.images)) 
55000 
print(len(mnist.test.images)) 
10000 
print(len(mnist.validation.images)) 
5000 
print(mnist.train.labels[1,:]) # The first label is a 3 
[ 0\.  0\.  0\.  1\.  0\.  0\.  0\.  0\.  0\.  0.] 
```

1.  垃圾邮件文本数据。 UCI 的机器学习数据集库还包含垃圾短信文本消息数据集。我们可以访问此`.zip`文件并获取垃圾邮件文本数据，如下所示：

```py
import requests 
import io 
from zipfile import ZipFile 
zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip' 
r = requests.get(zip_url) 
z = ZipFile(io.BytesIO(r.content)) 
file = z.read('SMSSpamCollection') 
text_data = file.decode() 
text_data = text_data.encode('ascii',errors='ignore') 
text_data = text_data.decode().split('\n') 
text_data = [x.split('\t') for x in text_data if len(x)>=1] 
[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)] 
print(len(text_data_train)) 
5574 
print(set(text_data_target)) 
{'ham', 'spam'} 
print(text_data_train[1]) 
Ok lar... Joking wif u oni... 
```

1.  电影评论数据：来自康奈尔大学的 Bo Pang 发布了一个电影评论数据集，将评论分为好或坏（3）。[您可以在以下网站上找到数据](http://www.cs.cornell.edu/people/pabo/movie-review-data/)。要下载，提取和转换此数据，我们可以运行以下代码：

```py
import requests 
import io 
import tarfile 
movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz' 
r = requests.get(movie_data_url) 
# Stream data into temp object 
stream_data = io.BytesIO(r.content) 
tmp = io.BytesIO() 
while True: 
    s = stream_data.read(16384) 
    if not s: 
        break 
    tmp.write(s) 
    stream_data.close() 
tmp.seek(0) 
# Extract tar file 
tar_file = tarfile.open(fileobj=tmp, mode="r:gz") 
pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos') 
neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg') 
# Save pos/neg reviews (Also deal with encoding) 
pos_data = [] 
for line in pos: 
    pos_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode()) 
neg_data = [] 
for line in neg: 
    neg_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode()) 
tar_file.close() 
print(len(pos_data)) 
5331 
print(len(neg_data)) 
5331 
# Print out first negative review 
print(neg_data[0]) 
simplistic , silly and tedious . 
```

1.  CIFAR-10 图像数据：加拿大高级研究院发布了一个图像集，其中包含 8000 万个带标签的彩色图像（每个图像缩放为 32 x 32 像素）。有 10 种不同的目标类别（飞机，汽车，鸟类等）。 CIFAR-10 是包含 60,000 张图像的子集。训练集中有 50,000 个图像，测试集中有 10,000 个。由于我们将以多种方式使用此数据集，并且因为它是我们较大的数据集之一，因此我们不会在每次需要时运行脚本。要获取此数据集，请导航至[此链接](http://www.cs.toronto.edu/~kriz/cifar.html)并下载 CIFAR-10 数据集。我们将在相应的章节中介绍如何使用此数据集。
2.  莎士比亚文本数据的作品：Project Gutenberg（5）是一个发布免费书籍电子版的项目。他们一起编辑了莎士比亚的所有作品。以下代码显示了如何通过 Python 访问此文本文件：

```py
import requests 
shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt' 
# Get Shakespeare text 
response = requests.get(shakespeare_url) 
shakespeare_file = response.content 
# Decode binary into string 
shakespeare_text = shakespeare_file.decode('utf-8') 
# Drop first few descriptive paragraphs. 
shakespeare_text = shakespeare_text[7675:] 
print(len(shakespeare_text)) # Number of characters 
5582212
```

1.  英语 - 德语句子翻译数据：[Tatoeba 项目](http://tatoeba.org) 收集多种语言的句子翻译。他们的数据已在 Creative Commons License 下发布。根据这些数据，[ManyThings.org](http://www.manythings.org) 编译了可供下载的文本文件中的句子到句子的翻译。在这里，我们将使用英语 - 德语翻译文件，但您可以将 URL 更改为您想要使用的语言：

```py
import requests 
import io 
from zipfile import ZipFile 
sentence_url = 'http://www.manythings.org/anki/deu-eng.zip' 
r = requests.get(sentence_url) 
z = ZipFile(io.BytesIO(r.content)) 
file = z.read('deu.txt') 
# Format Data 
eng_ger_data = file.decode() 
eng_ger_data = eng_ger_data.encode('ascii',errors='ignore') 
eng_ger_data = eng_ger_data.decode().split('\n') 
eng_ger_data = [x.split('\t') for x in eng_ger_data if len(x)>=1] 
[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)] 
print(len(english_sentence)) 
137673 
print(len(german_sentence)) 
137673 
print(eng_ger_data[10]) 
['I' won!, 'Ich habe gewonnen!'] 
```

## 工作原理

当在秘籍中使用这些数据集之一时，我们将引用您到本节并假设数据以上一节中描述的方式加载。如果需要进一步的数据转换或预处理，那么这些代码将在秘籍本身中提供。

## 另见

以下是我们在本书中使用的数据资源的其他参考：

*   Hosmer，D.W.，Lemeshow，S. 和 Sturdivant，R.X.，2013，Applied Logistic Regression：3rd Edition
*   [Lichman，M.，2013，UCI 机器学习库，Irvine，CA：加州大学信息与计算机科学学院](http://archive.ics.uci.edu/ml)
*   [Bo Pang，Lillian Lee 和 Shivakumar Vaithyanathan，竖起大拇指？使用机器学习技术的情感分类，EMNLP 2002 年会议录](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
*   [Krizhevsky，2009 年，从微小图像学习多层特征](http://www.cs.toronto.edu/~kriz/cifar.html)
*   [古腾堡项目，2016 年 4 月](http://www.gutenberg.org/)