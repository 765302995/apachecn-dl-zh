# 使用多个执行程序

您将意识到 TensorFlow 有许多功能，包括计算图，它们可以自然地并行计算。计算图可以分为不同的处理器以及处理不同的批量。我们将讨论如何在此秘籍中访问同一台机器上的不同处理器。

## 做好准备

对于此秘籍，我们将向您展示如何在同一系统上访问多个设备并对其进行训练。这是一种非常常见的情况：与 CPU 一起，机器可能具有一个或多个可以共享计算负载的 GPU。如果 TensorFlow 可以访问这些设备，它将通过贪婪的过程自动将计算分配给多个设备。但是，TensorFlow 还允许程序通过名称范围放置指定哪些设备将在哪个设备上。

要访问 GPU 设备，必须安装 GPU 版本的 TensorFlow。要安装 TensorFlow 的 GPU 版本，请访问[此链接](https://www.tensorflow.org/versions/master/get_started/os_setup.html)。下载，设置并按照特定系统的说明进行操作。请注意，TensorFlow 的 GPU 版本需要 CUDA 才能使用 GPU。

在本文中，我们将向您展示各种命令，允许您访问系统上的各种设备;我们还将演示如何找出 TensorFlow 正在使用的设备。

## 操作步骤

1.  为了找出 TensorFlow 用于哪些操作的设备，我们需要在会话参数中设置`config`，将`log_device_placement`设置为`True`。当我们从命令行运行脚本时，我们将看到特定的设备放置，如以下输出所示：

```py
import tensorflow as tf 
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) 
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a') 
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b') 
c = tf.matmul(a, b) 
# Runs the op. 
print(sess.run(c)) 
```

1.  从终端，运行以下命令：

```py
$python3 using_multiple_devices.py 
Device mapping: no known devices. 
I tensorflow/core/common_runtime/direct_session.cc:175] Device mapping: 
MatMul: /job:localhost/replica:0/task:0/cpu:0 
I tensorflow/core/common_runtime/simple_placer.cc:818] MatMul: /job:localhost/replica:0/task:0/cpu:0 
b: /job:localhost/replica:0/task:0/cpu:0 
I tensorflow/core/common_runtime/simple_placer.cc:818] b: /job:localhost/replica:0/task:0/cpu:0 
a: /job:localhost/replica:0/task:0/cpu:0 
I tensorflow/core/common_runtime/simple_placer.cc:818] a: /job:localhost/replica:0/task:0/cpu:0 
[[ 22\.  28.] 
 [ 49\.  64.]] 
```

1.  默认情况下，TensorFlow 会自动决定如何跨计算设备（CPU 和 GPU）分配计算，有时我们需要了解这些展示位置。这在加载早期的现有模型时非常有用，该模型在我们的计算机具有不同设备时在图中分配了硬展示位置。我们可以在配置中设置软放置以解决此问题，如下所示：

```py
config = tf.ConfigProto() 
config.allow_soft_placement = True 
sess_soft = tf.Session(config=config) 
```

1.  使用 GPU 时，TensorFlow 会自动占用 GPU 内存的很大一部分。虽然通常需要这样做，但我们可以采取措施更加小心 GPU 内存分配。虽然 TensorFlow 从未发布 GPU 内存，但我们可以通过设置 GPU 内存增长选项，将其分配缓慢增加到最大限制（仅在需要时），如下所示：

```py
config.gpu_options.allow_growth = True 
sess_grow = tf.Session(config=config) 
```

1.  如果我们想对 TensorFlow 使用的 GPU 内存百分比设置硬限制，我们可以使用`config`设置`per_process_gpu_memory_fraction`，如下所示：

```py
config.gpu_options.per_process_gpu_memory_fraction = 0.4 
sess_limited = tf.Session(config=config) 
```

1.  有时我们可能需要编写可靠的代码来确定它是否在 GPU 可用的情况下运行。 TensorFlow 具有内置功能，可以测试 GPU 是否可用。当我们想要编写在可用时利用 GPU 并为其分配特定操作的代码时，这很有用。这是通过以下代码完成的：

```py
if tf.test.is_built_with_cuda(): 
    <Run GPU specific code here>
```

1.  如果我们需要为 GPU 分配特定操作，请输入以下代码。这将执行简单的计算并将操作分配给主 CPU 和两个辅助 GPU：

```py
with tf.device('/cpu:0'): 
    a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3]) 
    b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1]) 

    with tf.device('/gpu:0'): 
        c = tf.matmul(a,b) 
        c = tf.reshape(c, [-1]) 

    with tf.device('/gpu:1'): 
        d = tf.matmul(b,a) 
        flat_d = tf.reshape(d, [-1]) 

    combined = tf.multiply(c, flat_d) 
print(sess.run(combined)) 
```

## 工作原理

当我们想在我们的机器上为 TensorFlow 操作指定特定设备时，我们需要知道 TensorFlow 如何引用这些设备。 TensorFlow 中的设备名称遵循以下约定：

| 设备 | 设备名称 |
| --- | --- | --- |
| 主 CPU | `/CPU:0` |
| 第二个 CPU | `/CPU:1` |
| 主 GPU | `/GPU:0` |
| 第二个 GPU | `/GPU:1` |
| 第三个 GPU | `/GPU:2` |

## 更多

幸运的是，在云中运行 TensorFlow 现在比以往更容易。许多云计算服务提供商都提供 GPU 实例，其中包含主 CPU 和强大的 GPU。 Amazon Web Services（AWS）具有 G 实例和 P2 实例，允许使用功能强大的 GPU，为 TensorFlow 流程提供极快的速度。您甚至可以免费选择 AWS Machine Images（AMI），它将在安装了 TensorFlow 的 GPU 实例的情况下启动选定的实例。