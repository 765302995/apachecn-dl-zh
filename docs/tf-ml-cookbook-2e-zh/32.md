# 使用线性 SVM

对于此示例，我们将从 iris 数据集创建线性分隔符。我们从前面的章节中知道，萼片长度和花瓣宽度创建了一个线性可分的二进制数据集，用于预测花是否是 I. setosa。

## 做好准备

要在 TensorFlow 中实现软可分 SVM，我们将实现特定的损失函数，如下所示：

![](img/11e1a288-7294-4c7c-bbee-37e854bbf309.png)

这里，`A`是部分斜率的向量，`b`是截距，`x[i]`是输入向量，`y[i]`是实际类，（-1 或 1），`α`是软可分性正则化参数。

## 操作步骤

我们按如下方式处理秘籍：

1.  我们首先加载必要的库。这将包括用于访问虹膜数据集的`scikit-learn`数据集库。使用以下代码：

```py
import matplotlib.pyplot as plt 
import numpy as np 
import tensorflow as tf 
from sklearn import datasets 
```

> 要为此练习设置 scikit-learn，我们只需要输入`$pip install -U scikit-learn`。请注意，它也安装了 Anaconda。

1.  接下来，我们启动图会话并根据需要加载数据。请记住，我们正在加载虹膜数据集中的第一个和第四个变量，因为它们是萼片长度和萼片宽度。我们正在加载目标变量，对于 I. setosa 将取值 1，否则为-1。使用以下代码：

```py
sess = tf.Session() 
iris = datasets.load_iris() 
x_vals = np.array([[x[0], x[3]] for x in iris.data]) 
y_vals = np.array([1 if y==0 else -1 for y in iris.target])
```

1.  我们现在应该将数据集拆分为训练集和测试集。我们将评估训练和测试集的准确率。由于我们知道这个数据集是线性可分的，因此我们应该期望在两个集合上获得 100％的准确率。要拆分数据，请使用以下代码：

```py
train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False) 
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices))) 
x_vals_train = x_vals[train_indices] 
x_vals_test = x_vals[test_indices] 
y_vals_train = y_vals[train_indices] 
y_vals_test = y_vals[test_indices] 
```

1.  接下来，我们设置批量大小，占位符和模型变量。值得一提的是，使用这种 SVM 算法，我们需要非常大的批量大小来帮助收敛。我们可以想象，对于非常小的批量大小，最大边际线会略微跳跃。理想情况下，我们也会慢慢降低学习率，但现在这已经足够了。此外，`A`变量将采用 2x1 形状，因为我们有两个预测变量：萼片长度和花瓣宽度。要进行此设置，我们使用以下代码：

```py
batch_size = 100 

x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32) 
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) 

A = tf.Variable(tf.random_normal(shape=[2,1])) 
b = tf.Variable(tf.random_normal(shape=[1,1])) 
```

1.  我们现在声明我们的模型输出。对于正确分类的点，如果目标是 I. setosa，则返回大于或等于 1 的数字，否则返回小于或等于-1。模型输出使用以下代码：

```py
model_output = tf.subtract(tf.matmul(x_data, A), b) 
```

1.  接下来，我们将汇总并声明必要的组件以获得最大的保证金损失。首先，我们将声明一个计算向量的 L2 范数的函数。然后，我们添加 margin 参数![](img/1f304159-a7af-498c-b746-72d49ecadecb.png)。然后我们宣布我们的分类损失并将这两个术语加在一起。使用以下代码：

```py
l2_norm = tf.reduce_sum(tf.square(A)) 
alpha = tf.constant([0.1]) 
classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(model_output, y_target)))) 

loss = tf.add(classification _term, tf.multiply(alpha, l2_norm)) 
```

1.  现在，我们声明我们的预测和准确率函数，以便我们可以评估训练集和测试集的准确率，如下所示：

```py
prediction = tf.sign(model_output) 
accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32)) 
```

1.  在这里，我们将声明我们的优化函数并初始化我们的模型变量;我们在以下代码中执行此操作：

```py
my_opt = tf.train.GradientDescentOptimizer(0.01) 
train_step = my_opt.minimize(loss) 

init = tf.global_variables_initializer() 
sess.run(init) 
```

1.  我们现在可以开始我们的训练循环，记住我们想要在训练和测试集上记录我们的损失和训练准确率，如下所示：

```py
loss_vec = [] 
train_accuracy = [] 
test_accuracy = [] 
for i in range(500): 
   rand_index = np.random.choice(len(x_vals_train), size=batch_size) 
   rand_x = x_vals_train[rand_index] 
    rand_y = np.transpose([y_vals_train[rand_index]]) 
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y}) 

    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y}) 
    loss_vec.append(temp_loss) 

    train_acc_temp = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])}) 
    train_accuracy.append(train_acc_temp) 

    test_acc_temp = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])}) 
    test_accuracy.append(test_acc_temp) 

    if (i+1)%100==0: 
        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) 
        print('Loss = ' + str(temp_loss))
```

1.  训练期间脚本的输出应如下所示：

```py
Step #100 A = [[-0.10763293] 
 [-0.65735245]] b = [[-0.68752676]] 
Loss = [ 0.48756418] 
Step #200 A = [[-0.0650763 ] 
 [-0.89443302]] b = [[-0.73912662]] 
Loss = [ 0.38910741] 
Step #300 A = [[-0.02090022] 
 [-1.12334013]] b = [[-0.79332656]] 
Loss = [ 0.28621092] 
Step #400 A = [[ 0.03189624] 
 [-1.34912157]] b = [[-0.8507266]] 
Loss = [ 0.22397576] 
Step #500 A = [[ 0.05958777] 
 [-1.55989814]] b = [[-0.9000265]] 
Loss = [ 0.20492229] 
```

1.  为了绘制输出（拟合，损失和精度），我们必须提取系数并将`x`值分成 I. setosa 和 Non-setosa，如下所示：

```py
[[a1], [a2]] = sess.run(A) 
[[b]] = sess.run(b) 
slope = -a2/a1 
y_intercept = b/a1 

x1_vals = [d[1] for d in x_vals] 

best_fit = [] 
for i in x1_vals: 
    best_fit.append(slope*i+y_intercept) 

setosa_x = [d[1] for i,d in enumerate(x_vals) if y_vals[i]==1] 
setosa_y = [d[0] for i,d in enumerate(x_vals) if y_vals[i]==1] 
not_setosa_x = [d[1] for i,d in enumerate(x_vals) if y_vals[i]==-1] 
not_setosa_y = [d[0] for i,d in enumerate(x_vals) if y_vals[i]==-1] 
```

1.  以下是使用线性分离器拟合，精度和损耗绘制数据的代码：

```py
plt.plot(setosa_x, setosa_y, 'o', label='I. setosa') 
plt.plot(not_setosa_x, not_setosa_y, 'x', label='Non-setosa') 
plt.plot(x1_vals, best_fit, 'r-', label='Linear Separator', linewidth=3) 
plt.ylim([0, 10]) 
plt.legend(loc='lower right') 
plt.title('Sepal Length vs Petal Width') 
plt.xlabel('Petal Width') 
plt.ylabel('Sepal Length') 
plt.show() 

plt.plot(train_accuracy, 'k-', label='Training Accuracy') 
plt.plot(test_accuracy, 'r--', label='Test Accuracy') 
plt.title('Train and Test Set Accuracies') 
plt.xlabel('Generation') 
plt.ylabel('Accuracy') 
plt.legend(loc='lower right') 
plt.show() 

plt.plot(loss_vec, 'k-') 
plt.title('Loss per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Loss') 
plt.show() 
```

> 以这种方式使用 TensorFlow 来实现 SVD 算法可能导致每次运行的结果略有不同。其原因包括随机训练/测试集拆分以及每个训练批次中不同批次点的选择。此外，在每一代之后慢慢降低学习率是理想的。

得到的图如下：

![](img/9c53587e-f2d7-4e1f-9af9-1bde936d24da.png)

图 2：最终线性 SVM 与绘制的两个类别拟合

![](img/fb331bce-9d8e-4d8e-928e-8446022dac2c.png)

图 3：迭代测试和训练集精度;我们确实获得 100％的准确率，因为这两个类是线性可分的

![](img/adb5aaf5-d575-4e93-a84c-7f709afcdcb6.png)

图 4：超过 500 次迭代的最大边际损失图

## 工作原理

在本文中，我们已经证明使用最大边际损失函数可以实现线性 SVD 模型。