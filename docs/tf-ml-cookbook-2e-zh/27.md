# 实现套索和岭回归

还有一些方法可以限制系数对回归输出的影响。这些方法称为正则化方法，两种最常见的正则化方法是套索和岭回归。我们将介绍如何在本文中实现这两个方面。

## 做好准备

套索和岭回归与常规线性回归非常相似，除了我们添加正则化项以限制公式中的斜率（或部分斜率）。这可能有多种原因，但一个常见的原因是我们希望限制对因变量产生影响的特征。这可以通过在损失函数中添加一个取决于我们的斜率值`A`的项来实现。

对于套索回归，如果斜率`A`超过某个值，我们必须添加一个能大大增加损失函数的项。我们可以使用 TensorFlow 的逻辑运算，但它们没有与之关联的梯度。相反，我们将使用称为连续重阶函数的阶梯函数的连续近似，该函数按比例放大到我们选择的正则化截止值。我们将展示如何在此秘籍中进行套索回归。

对于岭回归，我们只是在 L2 范数中添加一个项，这是斜率系数的缩放 L2 范数。这种修改很简单，并在本秘籍末尾的“更多...”部分中显示。

## 操作步骤

我们按如下方式处理秘籍：

1.  我们将再次使用 iris 数据集并以与以前相同的方式设置我们的脚本。我们先加载库;开始一个会议;加载数据;声明批量大小;并创建占位符，变量和模型输出，如下所示：

```py
import matplotlib.pyplot as plt 
import numpy as np 
import tensorflow as tf 
from sklearn import datasets 
from tensorflow.python.framework import ops 
ops.reset_default_graph() 
sess = tf.Session() 
iris = datasets.load_iris() 
x_vals = np.array([x[3] for x in iris.data]) 
y_vals = np.array([y[0] for y in iris.data]) 
batch_size = 50 
learning_rate = 0.001 
x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) 
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) 
A = tf.Variable(tf.random_normal(shape=[1,1])) 
b = tf.Variable(tf.random_normal(shape=[1,1])) 
model_output = tf.add(tf.matmul(x_data, A), b)
```

1.  我们添加了损失函数，它是一个改进的连续 Heaviside 阶梯函数。我们还为`0.9`设定了套索回归的截止值。这意味着我们希望将斜率系数限制为小于`0.9`。使用以下代码：

```py
lasso_param = tf.constant(0.9) 
heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-100., tf.subtract(A, lasso_param))))) 
regularization_param = tf.mul(heavyside_step, 99.) 
loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param) 
```

1.  我们现在初始化变量并声明我们的优化器，如下所示：

```py
init = tf.global_variables_initializer() 
sess.run(init) 
my_opt = tf.train.GradientDescentOptimizer(learning_rate) 
train_step = my_opt.minimize(loss) 
```

1.  我们将训练循环延长了一段时间，因为它可能需要一段时间才能收敛。我们可以看到斜率系数小于`0.9`。使用以下代码：

```py
loss_vec = [] 
for i in range(1500): 
    rand_index = np.random.choice(len(x_vals), size=batch_size) 
    rand_x = np.transpose([x_vals[rand_index]]) 
    rand_y = np.transpose([y_vals[rand_index]]) 
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y}) 
    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y}) 
    loss_vec.append(temp_loss[0]) 
    if (i+1)%300==0: 
        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) 
        print('Loss = ' + str(temp_loss)) 

Step #300 A = [[ 0.82512331]] b = [[ 2.30319238]] 
Loss = [[ 6.84168959]] 
Step #600 A = [[ 0.8200165]] b = [[ 3.45292258]] 
Loss = [[ 2.02759886]] 
Step #900 A = [[ 0.81428504]] b = [[ 4.08901262]] 
Loss = [[ 0.49081498]] 
Step #1200 A = [[ 0.80919558]] b = [[ 4.43668795]] 
Loss = [[ 0.40478843]] 
Step #1500 A = [[ 0.80433637]] b = [[ 4.6360755]] 
Loss = [[ 0.23839757]] 
```

## 工作原理

我们通过在线性回归的损失函数中添加连续的 Heaviside 阶跃函数来实现套索回归。由于阶梯函数的陡峭性，我们必须小心步长。步长太大而且不会收敛。对于岭回归，请参阅下一节中所需的更改。

## 更多

对于岭回归，我们将损失`ss`函数更改为如下：

```py
ridge_param = tf.constant(1.) 
ridge_loss = tf.reduce_mean(tf.square(A)) 
loss = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), 0) 
```