# 示例部分

现在让我们讨论有用的库和模块。

## TensorFlow 中的优化器方法 -- 训练模块

训练或参数优化阶段是机器学习工作流程的重要组成部分。

为此，TensorFlow 具有一个`tf.train`模块，该模块是一组对象的帮助程序，致力于实现数据科学家所需的各种不同优化策略。 此模块提供的主要对象称为优化器。

### `tf.train.Optimizer`类

`Optimizer`类允许您为`loss`函数计算梯度并将其应用于模型的不同变量。 在最著名的算法子类中，我们找到了梯度下降，Adam 和 Adagrad。

关于该类的一个主要提示是`Optimizer`类本身无法实例化。 子类之一。

如前所述，TensorFlow 允许您以符号方式定义函数，因此梯度也将以符号方式应用，从而提高了结果的准确率以及要应用于数据的操作的通用性。

为了使用`Optimizer`类，我们需要执行以下步骤：

1.  创建具有所需参数的`Optimizer`（在这种情况下为梯度下降）。

    ```py
            opt = GradientDescentOptimizer(learning_rate= [learning rate]) 

    ```

2.  为`cost`函数创建一个调用`minimize`方法的操作。

    ```py
            optimization_op = opt.minimize(cost, var_list=[variables list]) 

    ```

`minimize`方法具有以下形式：

```py
tf.train.Optimizer.minimize(loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None) 

```

主要参数如下：

*   `loss`：这是一个张量，其中包含要最小化的值。
*   `global_step`：`Optimizer`工作后，此变量将增加 1。
*   `var_list`：包含要优化的变量。

### 提示

实际上，`optimize`方法结合了对`compute_gradients()`和`apply_gradients()`的调用。 如果要在应用梯度之前对其进行处理，请显式调用`compute_gradients()`和`apply_gradients()`，而不要使用此函数。 如果我们只想进行一步训练，就必须以`opt_op.run().`的形式执行`run`方法

### 其他优化器实例类型

以下是其他`Optimizer`实例类型：

*   `tf.train.AdagradOptimizer`：这是一种基于参数频率的自适应方法，学习率单调下降。
*   `tf.train.AdadeltaOptimizer`：这是对 Adagrad 的改进，它的学习率没有下降。
*   `tf.train.MomentumOptimizer`：这是一种适应性方法，可解决尺寸之间的不同变化率。
*   并且还有其他更具体的参数，例如`tf.train.AdamOptimizer`，`tf.train.FtrlOptimizer`和`tf.train.RMSPropOptimizer`。