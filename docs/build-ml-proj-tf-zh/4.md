# 基本张量方法

在本节中，我们将探索 TensorFlow 支持的一些基本方法。 它们对于初始数据探索和为更好的并行计算准备数据很有用。

## 简单矩阵运算

TensorFlow 支持许多更常见的矩阵运算，例如转置，乘法，获取行列式和逆运算。

这是应用于样本数据的那些函数的一个小例子：

```py
In [1]: import tensorflow as tf 
In [2]: sess = tf.InteractiveSession() 
In [3]: x = tf.constant([[2, 5, 3, -5], 
...:                  [0, 3,-2,  5], 
...:                  [4, 3, 5,  3], 
...:                  [6, 1, 4,  0]]) 

In [4]: y = tf.constant([[4, -7, 4, -3, 4], 
...:                  [6, 4,-7,  4, 7], 
...:                  [2, 3, 2,  1, 4], 
...:                  [1, 5, 5,  5, 2]]) 
In [5]: floatx = tf.constant([[2., 5., 3., -5.], 
...:                       [0., 3.,-2.,  5.], 
...:                       [4., 3., 5.,  3.], 
...:                       [6., 1., 4.,  0.]]) 
In [6]: tf.transpose(x).eval() # Transpose matrix 
Out[6]: 
array([[ 2,  0,  4,  6], 
[ 5,  3,  3,  1], 
[ 3, -2,  5,  4], 
[-5,  5,  3,  0]], dtype=int32) 

In [7]: tf.matmul(x, y).eval() # Matrix multiplication 
Out[7]: 
array([[ 39, -10, -46,  -8,  45], 
[ 19,  31,   0,  35,  23], 
[ 47,  14,  20,  20,  63], 
[ 38, -26,  25, -10,  47]], dtype=int32) 

In [8]: tf.matrix_determinant(floatx).eval() # Matrix determinant 
Out[8]: 818.0 

In [9]: tf.matrix_inverse(floatx).eval() # Matrix inverse 
Out[9]: 
array([[-0.00855745,  0.10513446, -0.18948655,  0.29584351], 
[ 0.12958434,  0.12224938,  0.01222495, -0.05134474], 
[-0.01955992, -0.18826403,  0.28117359, -0.18092911], 
[-0.08557458,  0.05134474,  0.10513448, -0.0415648 ]], dtype=float32) 

In [10]: tf.matrix_solve(floatx, [[1],[1],[1],[1]]).eval() # Solve Matrix system 
Out[10]: 
array([[ 0.20293398], 
[ 0.21271393], 
[-0.10757945], 
[ 0.02933985]], dtype=float32) 

```

### 归约

归约运算是对张量的一个维度进行运算的操作，而其维数较小。

支持的操作（具有相同参数）包括乘积，最小值，最大值，平均值，所有，任意和 accumulate_n）。

```py
In [1]: import tensorflow as tf 

In [2]: sess = tf.InteractiveSession() 
In [3]: x = tf.constant([[1,  2, 3], 
...:                  [3,  2, 1], 
...:                  [-1,-2,-3]]) 
In [4]: 

In [4]: boolean_tensor = tf.constant([[True,  False, True], 
...:                  [False, False, True], 
...:                  [True, False, False]]) 

In [5]: tf.reduce_prod(x, reduction_indices=1).eval() # reduce prod 
Out[5]: array([ 6,  6, -6], dtype=int32) 

In [6]: tf.reduce_min(x, reduction_indices=1).eval() # reduce min 
Out[6]: array([ 1,  1, -3], dtype=int32) 

In [7]: tf.reduce_max(x, reduction_indices=1).eval() # reduce max 
Out[7]: array([ 3,  3, -1], dtype=int32) 

In [8]: tf.reduce_mean(x, reduction_indices=1).eval() # reduce mean 
Out[8]: array([ 2,  2, -2], dtype=int32) 

In [9]: tf.reduce_all(boolean_tensor, reduction_indices=1).eval() # reduce all 
Out[9]: array([False, False, False], dtype=bool) 

In [10]: tf.reduce_any(boolean_tensor, reduction_indices=1).eval() # reduce any 
Out[10]: array([ True,  True,  True], dtype=bool) 

```

### 张量分段

张量分段是一个过程，其中某个维度被归约，并且所得元素由索引行确定。 如果该行中的某些元素被重复，则对应的索引将转到其中的值，并且该操作将在具有重复索引的索引之间应用。

索引数组的大小应与索引数组的维度 0 的大小相同，并且必须增加 1。

![Tensor segmentation](img/00007.jpg)

细分说明（重做）

```py
In [1]: import tensorflow as tf 
In [2]: sess = tf.InteractiveSession() 
In [3]: seg_ids = tf.constant([0,1,1,2,2]); # Group indexes : 0|1,2|3,4 
In [4]: tens1 = tf.constant([[2, 5, 3, -5], 
...:                     [0, 3,-2,  5], 
...:                     [4, 3, 5,  3], 
...:                     [6, 1, 4,  0], 
...:                     [6, 1, 4,  0]])  # A sample constant matrix 

In [5]: tf.segment_sum(tens1, seg_ids).eval()   # Sum segmentation 
Out[5]: 
array([[ 2,  5,  3, -5], 
[ 4,  6,  3,  8], 
[12,  2,  8,  0]], dtype=int32) 

In [6]: tf.segment_prod(tens1, seg_ids).eval() # Product segmentation 
Out[6]: 
array([[  2,   5,   3,  -5], 
[  0,   9, -10,  15], 
[ 36,   1,  16,   0]], dtype=int32) 

In [7]: tf.segment_min(tens1, seg_ids).eval() # minimun value goes to group 
Out[7]: 
array([[ 2,  5,  3, -5], 
[ 0,  3, -2,  3], 
[ 6,  1,  4,  0]], dtype=int32) 

In [8]: tf.segment_max(tens1, seg_ids).eval() # maximum value goes to group 
Out[8]: 
array([[ 2,  5,  3, -5], 
[ 4,  3,  5,  5], 
[ 6,  1,  4,  0]], dtype=int32) 

In [9]: tf.segment_mean(tens1, seg_ids).eval() # mean value goes to group 
Out[9]: 
array([[ 2,  5,  3, -5], 
[ 2,  3,  1,  4], 
[ 6,  1,  4,  0]], dtype=int32) 

```

## 序列

序列实用程序包括诸如`argmin`和`argmax`（显示维度的最小值和最大值），`listdiff`（显示列表之间交集的补码），`where`（显示真实值的索引）和`unique`（在列表上显示唯一值）之类的张量方法。

```py
In [1]: import tensorflow as tf 
In [2]: sess = tf.InteractiveSession() 
In [3]: x = tf.constant([[2, 5, 3, -5], 
...:                  [0, 3,-2,  5], 
...:                  [4, 3, 5,  3], 
...:                  [6, 1, 4,  0]]) 
In [4]: listx = tf.constant([1,2,3,4,5,6,7,8]) 
In [5]: listy = tf.constant([4,5,8,9]) 

In [6]: 

In [6]: boolx = tf.constant([[True,False], [False,True]]) 

In [7]: tf.argmin(x, 1).eval() # Position of the maximum value of columns 
Out[7]: array([3, 2, 1, 3]) 

In [8]: tf.argmax(x, 1).eval() # Position of the minimum value of rows 
Out[8]: array([1, 3, 2, 0]) 

In [9]: tf.listdiff(listx, listy)[0].eval() # List differences 
Out[9]: array([1, 2, 3, 6, 7], dtype=int32) 

In [10]: tf.where(boolx).eval() # Show true values 
Out[10]: 
array([[0, 0], 
[1, 1]]) 

In [11]: tf.unique(listx)[0].eval() # Unique values in list 
Out[11]: array([1, 2, 3, 4, 5, 6, 7, 8], dtype=int32) 

```

## 张量形状变换

这些操作与矩阵形状有关，用于调整不匹配的数据结构并检索有关数据量度的快速信息。 这在确定运行时的处理策略时很有用。

在以下示例中，我们将从第二张量张量开始，并将打印有关它的一些信息。

然后，我们将探讨按维度修改矩阵的操作，包括添加或删除维度，例如`squeeze`和`expand_dims`：

```py
In [1]: import tensorflow as tf 
In [2]: sess = tf.InteractiveSession() 
In [3]: x = tf.constant([[2, 5, 3, -5], 
...:                  [0, 3,-2,  5], 
...:                  [4, 3, 5,  3], 
...:                  [6, 1, 4,  0]]) 

In [4]: tf.shape(x).eval() # Shape of the tensor 
Out[4]: array([4, 4], dtype=int32) 

In [5]: tf.size(x).eval() # size of the tensor 
Out[5]: 16 

In [6]: tf.rank(x).eval() # rank of the tensor 
Out[6]: 2 

In [7]: tf.reshape(x, [8, 2]).eval() # converting to a 10x2 matrix 
Out[7]: 
array([[ 2,  5], 
[ 3, -5], 
[ 0,  3], 
[-2,  5], 
[ 4,  3], 
[ 5,  3], 
[ 6,  1], 
[ 4,  0]], dtype=int32) 

In [8]: tf.squeeze(x).eval() #  squeezing 
Out[8]: 
array([[ 2,  5,  3, -5], 
[ 0,  3, -2,  5], 
[ 4,  3,  5,  3], 
[ 6,  1,  4,  0]], dtype=int32) 

In [9]: tf.expand_dims(x,1).eval() #Expanding dims 
Out[9]: 
array([[[ 2,  5,  3, -5]], 
[[ 0,  3, -2,  5]], 
[[ 4,  3,  5,  3]], 
[[ 6,  1,  4,  0]]], dtype=int32) 

```

### 张量切片和合并

为了从大型数据集中提取和合并有用的信息，切片和联接方法使您可以合并所需的列信息，而不必使用非特定信息来占用内存空间。

在以下示例中，我们将提取矩阵切片，对其进行分割，添加填充以及对行进行打包和解包：

```py
In [1]: import tensorflow as tf 
In [2]: sess = tf.InteractiveSession() 
In [3]: t_matrix = tf.constant([[1,2,3], 
...:                         [4,5,6], 
...:                         [7,8,9]]) 
In [4]: t_array = tf.constant([1,2,3,4,9,8,6,5]) 
In [5]: t_array2= tf.constant([2,3,4,5,6,7,8,9]) 

In [6]: tf.slice(t_matrix, [1, 1], [2,2]).eval() # cutting an slice 
Out[6]: 
array([[5, 6], 
[8, 9]], dtype=int32) 

In [7]: tf.split(0, 2, t_array) # splitting the array in two 
Out[7]: 
[<tf.Tensor 'split:0' shape=(4,) dtype=int32>, 
<tf.Tensor 'split:1' shape=(4,) dtype=int32>] 

In [8]: tf.tile([1,2],[3]).eval() # tiling this little tensor 3 times 
Out[8]: array([1, 2, 1, 2, 1, 2], dtype=int32) 

In [9]: tf.pad(t_matrix, [[0,1],[2,1]]).eval() # padding 
Out[9]: 
array([[0, 0, 1, 2, 3, 0], 
[0, 0, 4, 5, 6, 0], 
[0, 0, 7, 8, 9, 0], 
[0, 0, 0, 0, 0, 0]], dtype=int32) 

In [10]: tf.concat(0, [t_array, t_array2]).eval() #concatenating list 
Out[10]: array([1, 2, 3, 4, 9, 8, 6, 5, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32) 

In [11]: tf.pack([t_array, t_array2]).eval() # packing 
Out[11]: 
array([[1, 2, 3, 4, 9, 8, 6, 5], 
[2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32) 

In [12]: sess.run(tf.unpack(t_matrix)) # Unpacking, we need the run method to view the tensors 
Out[12]: 
[array([1, 2, 3], dtype=int32), 
array([4, 5, 6], dtype=int32), 
array([7, 8, 9], dtype=int32)] 

In [13]: tf.reverse(t_matrix, [False,True]).eval() # Reverse matrix 
Out[13]: 
array([[3, 2, 1], 
[6, 5, 4], 
[9, 8, 7]], dtype=int32) 

```

## 数据流结构和结果可视化 -- TensorBoard

可视化摘要信息是任何数据科学家工具箱的重要组成部分。

TensorBoard 是一个软件实用程序，它允许数据流图的图形表示和用于解释结果的仪表板，通常来自日志记录实用程序：

![Dataflow structure and results visualization - TensorBoard](img/00008.jpg)

TensorBoard GUI

可以将图的所有张量和操作设置为将信息写入日志。 TensorBoard 分析在 Session 运行时正常编写的信息，并向用户显示许多图形项，每个图形项一个。

### 命令行使用

要调用 TensorBoard，命令行为：

![Command line use](img/00009.jpg)

## TensorBoard 的工作方式

我们构建的每个计算图都有 TensorFlow 的实时日志记录机制，以便保存模型拥有的几乎所有信息。

但是，模型构建者必须考虑应保存的几百个信息维中的哪一个，以后才能用作分析工具。

为了保存所有必需的信息，TensorFlow API 使用了称为摘要的数据输出对象。

这些摘要将结果写入 TensorFlow 事件文件，该文件收集在 Session 运行期间生成的所有必需数据。

在以下示例中，我们将直接在生成的事件日志目录上运行 TensorBoard：

![How TensorBoard works](img/00010.jpg)

### 添加摘要节点

TensorFlow 会话中的所有摘要均由`SummaryWriter`对象编写。 调用的主要方法是：

```py
tf.train.SummaryWriter.__init__(logdir, graph_def=None) 

```

该命令将在参数的路径中创建一个`SummaryWriter`和一个事件文件。

`SummaryWriter`的构造函数将在`logdir`中创建一个新的事件文件。 当您调用以下函数之一时，此事件文件将包含`Event`类型的协议缓冲区：`add_summary()`，`add_session_log()`，`add_event()`或`add_graph()`。

如果将`graph_def`协议缓冲区传递给构造函数，则会将其添加到事件文件中。 （这等效于稍后调用`add_graph()`）。

当您运行 TensorBoard 时，它将从文件中读取图定义并以图形方式显示它，以便您可以与其进行交互。

首先，创建您要从中收集摘要数据的 TensorFlow 图，并确定要使用摘要操作注释的节点。

TensorFlow 中的操作在您运行它们或取决于它们的输出的操作之前不会做任何事情。 我们刚刚创建的摘要节点是图的外围：当前运行的所有操作都不依赖于它们。 因此，要生成摘要，我们需要运行所有这些摘要节点。 手动管理它们很繁琐，因此请使用`tf.merge_all_summaries`将它们组合为一个可生成所有摘要数据的操作。

然后，您可以运行合并的摘要操作，这将在给定步骤中生成一个包含所有摘要数据的序列化摘要`protobuf`对象。 最后，要将摘要数据写入磁盘，请将摘要`protobuf`传递给`tf.train.SummaryWriter`。

`SummaryWriter`在其构造函数中带有`logdir`，此`logdir`非常重要，它是所有事件将被写出的目录。 同样，`SummaryWriter`可以选择在其构造函数中使用`GraphDef`。 如果收到一个，TensorBoard 还将可视化您的图。

现在，您已经修改了图并具有`SummaryWriter`，就可以开始运行网络了！ 如果需要，您可以在每个步骤中运行合并的摘要操作，并记录大量的训练数据。 不过，这可能是您需要的更多数据。 相反，请考虑每 n 个步骤运行一次合并的摘要操作。

### 通用摘要操作

这是不同的摘要类型及其构造所使用的参数的列表：

*   `tf.scalar_summary`（标签，值，集合=无，名称=无）
*   `tf.image_summary`（标签，张量，max_images = 3，集合=无，名称=无）
*   `tf.histogram_summary`（标签，值，集合=无，名称=无）

### 特殊摘要函数

这些是特殊函数，用于合并不同操作的值，无论是摘要的集合，还是图中的所有摘要：

*   `tf.merge_summary`（输入，集合=无，名称=无）
*   `tf.merge_all_summaries`（键=“摘要”）

最后，作为提高可读性的最后一项帮助，可视化对常数和汇总节点使用特殊的图标。 总而言之，这是节点符号表：

| 符号 | 含义 |
| --- | --- |
| ![Special Summary functions](img/00011.jpg) | 代表名称范围的高级节点。 双击以展开一个高级节点。 |
| ![Special Summary functions](img/00012.jpg) | 彼此不连接的编号节点序列。 |
| ![Special Summary functions](img/00013.jpg) | 彼此连接的编号节点序列。 |
| ![Special Summary functions](img/00014.jpg) | 单个操作节点。 |
| ![Special Summary functions](img/00015.jpg) | 一个常数。 |
| ![Special Summary functions](img/00016.jpg) | 摘要节点。 |
| ![Special Summary functions](img/00017.jpg) | 边缘显示操作之间的数据流。 |
| ![Special Summary functions](img/00018.jpg) | 边缘显示操作之间的控制依赖性。 |
| ![Special Summary functions](img/00019.jpg) | 显示输出操作节点可以改变输入张量的参考边。 |

### 与 TensorBoard 的 GUI 交互

通过平移和缩放来浏览图形。单击并拖动以进行平移，然后使用滚动手势进行缩放。 双击节点，或单击其`+`按钮，以展开表示操作代码的名称范围。 为了轻松跟踪缩放时的当前视点，右下角有一个小地图：

![Interacting with TensorBoard's GUI](img/00020.jpg)

具有一个扩展的操作组和图例的 Openflow

要关闭打开的节点，请再次双击它或单击其`-`按钮。 您也可以单击一次以选择一个节点。 它将变为较暗的颜色，有关该颜色及其连接的节点的详细信息将显示在可视化文件右上角的信息卡中。

选择还有助于理解高级节点。 选择任何高度节点，其他连接的相应节点图标也会被选择。 例如，这可以轻松查看正在保存的节点和未保存的节点。

单击信息卡中的节点名称将其选中。 如有必要，视点将自动平移以使该节点可见。

最后，您可以使用图例上方的颜色菜单为图形选择两种配色方案。 默认的“结构视图”显示结构：当两个高级节点具有相同的结构时，它们以相同的彩虹色显示。 唯一结构化的节点为灰色。 第二个视图显示了不同操作在哪个设备上运行。 名称范围的颜色与设备中用于其内部操作的部分的比例成比例。

## 从磁盘读取信息

TensorFlow 读取许多最标准的格式，包括众所周知的 CSV，图像文件（JPG 和 PNG 解码器）以及标准 TensorFlow 格式。

### 列表格式-CSV

为了读取众所周知的 CSV 格式，TensorFlow 有自己的方法。 与其他库（例如熊猫）相比，读取简单 CSV 文件的过程稍微复杂一些。

读取 CSV 文件需要完成前面的几个步骤。 首先，我们必须使用要使用的文件列表创建文件名队列对象，然后创建`TextLineReader`。 使用此行读取器，剩下的操作将是解码 CSV 列，并将其保存在张量上。 如果我们想将同类数据混合在一起，则 pack 方法将起作用。

#### 鸢尾花数据集

鸢尾花数据集或费舍尔鸢尾花数据集是分类问题的众所周知基准。 这是罗纳德·费舍尔（Ronald Fisher）在 1936 年的论文中引入的多元数据集，该分类法是将生物分类问题中的多次测量用作线性判别分析的示例。

数据集包含来自三种鸢尾花（鸢尾鸢尾，初春鸢尾和杂色鸢尾）中每种的 50 个样本。 在每个样本中测量了四个特征：萼片和花瓣的长度和宽度，以厘米为单位。 基于这四个特征的组合，Fisher 开发了一个线性判别模型以区分物种。 （您可以在书的代码包中获取此数据集的`.csv`文件。）

为了读取 CSV 文件，您必须下载它并将其放在与 Python 可执行文件运行所在的目录中。

在下面的代码示例中，我们将从知名的 Iris 数据库中读取和打印前五个记录：

```py
import tensorflow as tf 
sess = tf.Session() 
filename_queue = tf.train.string_input_producer( 
tf.train.match_filenames_once("./*.csv"), 
shuffle=True) 
reader = tf.TextLineReader(skip_header_lines=1) 
key, value = reader.read(filename_queue) 
record_defaults = [[0.], [0.], [0.], [0.], [""]] 
col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)  # Convert CSV records to tensors. Each column maps to one tensor. 
features = tf.pack([col1, col2, col3, col4]) 

tf.initialize_all_variables().run(session=sess) 
coord = tf.train.Coordinator() 
threads = tf.train.start_queue_runners(coord=coord, sess=sess) 

for iteration in range(0, 5):
 example = sess.run([features])
 print(example)
 coord.request_stop()
 coord.join(threads)

```

这就是输出的样子：

![The Iris dataset](img/00021.jpg)

### 读取图像数据

TensorFlow 允许从图像格式导入数据，这对于导入面向图像的模型的自定义图像输入将非常有用。可接受的图像格式将为 JPG 和 PNG，内部表示形式为`uint8`张量，每个张量为图片通道的二阶张量：

![Reading image data](img/00022.jpg)

要读取的样本图像

### 加载和处理图像

在此示例中，我们将加载示例图像并对其进行一些其他处理，将生成的图像保存在单独的文件中：

```py
import tensorflow as tf 
sess = tf.Session() 
filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once("./blue_jay.jpg")) 
reader = tf.WholeFileReader() 
key, value = reader.read(filename_queue) 
image=tf.image.decode_jpeg(value) 
flipImageUpDown=tf.image.encode_jpeg(tf.image.flip_up_down(image)) 
flipImageLeftRight=tf.image.encode_jpeg(tf.image.flip_left_right(image)) 
tf.initialize_all_variables().run(session=sess) 
coord = tf.train.Coordinator() 
threads = tf.train.start_queue_runners(coord=coord, sess=sess) 
example = sess.run(flipImageLeftRight) 
print example 
file=open ("flippedUpDown.jpg", "wb+") 
file.write (flipImageUpDown.eval(session=sess)) 
file.close() 
file=open ("flippedLeftRight.jpg", "wb+") 
file.write (flipImageLeftRight.eval(session=sess)) 
file.close() 

```

`print example`行将逐行显示图像中 RGB 值的摘要：

![Loading and processing the images](img/00023.jpg)

最终图像如下所示：

![Loading and processing the images](img/00024.jpg)

比较原始图像和变更后的图像（flipUpDown 和 flipLeftRight）

### 从标准 TensorFlow 格式读取

另一种方法是将您拥有的任意数据转换为正式格式。 这种方法使混合和匹配数据集和网络架构变得更加容易。

您可以编写一个获取数据的小程序，将其填充到示例协议缓冲区中，将协议缓冲区序列化为字符串，然后使用`tf.python_io.TFRecordWriter`类将字符串写入`TFRecords`文件。

要读取`TFRecords`的文件，请将`tf.TFRecordReader`与`tf.parse_single_example`解码器一起使用。 `parse_single_example` `op`将示例协议缓冲区解码为张量。