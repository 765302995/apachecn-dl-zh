# 第三个项目 -- 学习葡萄酒分类：多类分类

在本节中，我们将使用更复杂的数据集，尝试根据产地对葡萄酒进行分类。

## 数据集说明和加载

该数据包含对来自意大利同一地区但来自三个不同品种的葡萄酒进行化学分析的结果。 分析确定了三种葡萄酒中每种所含 13 种成分的数量。

数据变量：

*   醇
*   苹果酸
*   灰
*   灰的碱度
*   镁
*   总酚
*   黄酮
*   非类黄酮酚
*   花青素
*   色彩强度
*   色调
*   稀释酒的 OD280 / OD315
*   脯氨酸

要读取数据集，我们将仅使用提供的 CSV 文件和熊猫：

```py
df = pd.read_csv("./wine.csv", header=0)
```

![Dataset description and loading](img/00089.jpg)

## 数据集预处理

随着 csv 上的值从 1 开始，我们将归一化带有偏差的值：

```py
y = df['Wine'].values-1 

```

对于结果，我们将这些选项表示为一个数组的热门列表：

```py
Y = tf.one_hot(indices = y, depth=3, on_value = 1., off_value = 0., axis = 1 , name = "a").eval() 

```

我们还将预先洗净值：

```py
X, Y = shuffle (X, Y) 
scaler = preprocessing.StandardScaler() 
X = scaler.fit_transform(X) 

```

## 建模架构

这个特定的模型将由一个单层，全连接的神经网络组成：

*   `x` = `tf.placeholder(tf.float32, [None, 12])`
*   `W` = `tf.Variable(tf.zeros([12, 3]))`
*   `b` = `tf.Variable(tf.zeros([3]))`
*   `y` = `tf.nn.softmax(tf.matmul(x, W) + b)`

## 损失函数说明

我们将使用交叉熵函数来衡量损失：

```py
y_ = tf.placeholder(tf.float32, [None, 3]) 
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) 

```

## 损失函数优化器

同样，将使用“梯度下降”方法来减少损失函数：

```py
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) 

```

## 收敛测试

在收敛性测试中，我们将每个良好的回归均转换为 1，将每个错误的回归均转换为 0，然后获取值的平均值来衡量模型的准确率：

```py
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 
print(accuracy.eval({x: Xt, y_: Yt})) 

```

## 结果描述

如我们所见，随着历时的发展，我们具有可变精度，但是它总是优于 90％的精度，具有 30％的随机基数（如果我们生成 0 到 3 之间的随机数来猜测结果）。

```py
0.973684
0.921053
0.921053
0.947368
0.921053
```

## 完整源代码

让我们看一下完整的源代码：

```py
sess = tf.InteractiveSession() 
import pandas as pd 
# Import data 
from tensorflow.examples.tlutorials.mnist import input_data 
from sklearn.utils import shuffle 
import tensorflow as tf 

from sklearn import preprocessing 

flags = tf.app.flags 
FLAGS = flags.FLAGS 

df = pd.read_csv("./wine.csv", header=0) 
print (df.describe()) 
#df['displacement']=df['displacement'].astype(float) 
X = df[df.columns[1:13]].values 
y = df['Wine'].values-1 
Y = tf.one_hot(indices = y, depth=3, on_value = 1., off_value = 0., axis = 1 , name = "a").eval() 
X, Y = shuffle (X, Y) 

scaler = preprocessing.StandardScaler() 
X = scaler.fit_transform(X) 

# Create the model 
x = tf.placeholder(tf.float32, [None, 12]) 
W = tf.Variable(tf.zeros([12, 3])) 
b = tf.Variable(tf.zeros([3])) 
y = tf.nn.softmax(tf.matmul(x, W) + b) 

# Define loss and optimizer 
y_ = tf.placeholder(tf.float32, [None, 3]) 
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) 
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) 
# Train 
tf.initialize_all_variables().run() 
for i in range(100): 
X,Y =shuffle (X, Y, random_state=1) 

Xtr=X[0:140,:] 
Ytr=Y[0:140,:] 

Xt=X[140:178,:] 
Yt=Y[140:178,:] 
Xtr, Ytr = shuffle (Xtr, Ytr, random_state=0) 
#batch_xs, batch_ys = mnist.train.next_batch(100) 
batch_xs, batch_ys = Xtr , Ytr 
train_step.run({x: batch_xs, y_: batch_ys}) 
cost = sess.run (cross_entropy, feed_dict={x: batch_xs, y_: batch_ys}) 
# Test trained model 
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 
print(accuracy.eval({x: Xt, y_: Yt})) 

```