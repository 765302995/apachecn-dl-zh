# 第一个项目 -- 非线性合成函数回归

人工神经网络示例通常包含绝大多数分类问题，但实际上有大量应用可以表示为回归。

用于回归的网络架构与用于分类问题的网络架构没有很大不同：它们可以采用多变量输入，也可以使用线性和非线性激活函数。

在某些情况下，唯一必要的情况是仅在层的末尾删除类似于 S 形的函数，以允许出现所有选项。

在第一个示例中，我们将对一个简单的，有噪声的二次函数进行建模，并将尝试通过单个隐藏层网络对其进行回归，并查看我们可以多么接近地预测从测试总体中得出的值。

## 数据集说明和加载

在这种情况下，我们将使用生成的数据集，该数据集与第 3 章的线性回归中的数据集非常相似。

我们将使用常见的 Numpy 方法生成二次函数，然后添加随机噪声，这将有助于我们了解线性回归如何推广。

核心样本创建例程如下：

```py
import numpy as np 
trainsamples = 200 
testsamples = 60 
dsX = np.linspace(-1, 1, trainsamples + testsamples).transpose() 
dsY = 0.4* pow(dsX,2) +2 * dsX + np.random.randn(*dsX.shape) * 0.22 + 0.8  

```

## 数据集预处理

该数据集在生成时不需要进行预处理，并且具有良好的属性，例如居中并具有-1，1 x 样本分布。

## 建模架构 -- 损失函数描述

此设置的损耗将简单地用均方根误差表示，如下所示：

```py
cost = tf.pow(py_x-Y, 2)/(2)  

```

## 损失函数优化器

在这种情况下，我们将使用 Gradient Descent 成本优化器，可以通过以下代码调用该优化器：

```py
train_op = tf.train.AdamOptimizer(0.5).minimize(cost)  

```

## 准确率和收敛性测试

`predict_op = tf.argmax(py_x, 1)`

```py
cost1 += sess.run(cost, feed_dict={X: [[x1]], Y: y1}) / testsamples 

```

### 示例代码

让我们看一下下面显示的示例代码：

```py
import tensorflow as tf
import numpy as np
from sklearn.utils import shuffle
%matplotlib inline
import matplotlib.pyplot as plt
trainsamples = 200
testsamples = 60
#Here we will represent the model, a simple imput, a hidden layer of sigmoid activation
def model(X, hidden_weights1, hidden_bias1, ow):
    hidden_layer =  tf.nn.sigmoid(tf.matmul(X, hidden_weights1)+ b)
    return tf.matmul(hidden_layer, ow)  
dsX = np.linspace(-1, 1, trainsamples + testsamples).transpose()
dsY = 0.4* pow(dsX,2) +2 * dsX + np.random.randn(*dsX.shape) * 0.22 + 0.8
plt.figure() # Create a new figure
plt.title('Original data')
plt.scatter(dsX,dsY) #Plot a scatter draw of the datapoints
```

![Example code](img/00086.jpg)

```py
X = tf.placeholder("float")
Y = tf.placeholder("float")
# Create first hidden layer
hw1 = tf.Variable(tf.random_normal([1, 10], stddev=0.1))
# Create output connection
ow = tf.Variable(tf.random_normal([10, 1], stddev=0.0))
# Create bias
b = tf.Variable(tf.random_normal([10], stddev=0.1))
model_y = model(X, hw1, b, ow)
# Cost function
cost = tf.pow(model_y-Y, 2)/(2)
# construct an optimizer
train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)
# Launch the graph in a session
with tf.Session() as sess:
    tf.initialize_all_variables().run() #Initialize all variables
    for i in range(1,100):
        dsX, dsY = shuffle (dsX.transpose(), dsY) #We randomize the samples to mplement a better training
        trainX, trainY =dsX[0:trainsamples], dsY[0:trainsamples]
        for x1,y1 in zip (trainX, trainY):
            sess.run(train_op, feed_dict={X: [[x1]], Y: y1})
        testX, testY = dsX[trainsamples:trainsamples + testsamples], dsY[0:trainsamples:trainsamples+testsamples]
        cost1=0.
        for x1,y1 in zip (testX, testY):
            cost1 += sess.run(cost, feed_dict={X: [[x1]], Y: y1}) / testsamples      
        if (i%10 == 0):
            print "Average cost for epoch " + str (i) + ":" + str(cost1)
```

## 结果描述

这是不同周期的结果的副本。请注意，由于这是一个非常简单的函数，因此即使第一次迭代也具有非常好的结果：

```py
Average cost for epoch 1:[[ 0.00753353]]
Average cost for epoch 2:[[ 0.00381996]]
Average cost for epoch 3:[[ 0.00134867]]
Average cost for epoch 4:[[ 0.01020064]]
Average cost for epoch 5:[[ 0.00240157]]
Average cost for epoch 6:[[ 0.01248318]]
Average cost for epoch 7:[[ 0.05143405]]
Average cost for epoch 8:[[ 0.00621457]]
Average cost for epoch 9:[[ 0.0007379]]
```