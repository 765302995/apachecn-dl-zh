# 示例 2 -- 编写音乐 “a la” Bach

在此示例中，我们将使用专门针对字符序列或 char RNN 模型的循环神经网络。

我们将使用一系列基于字符的格式表达的音乐，即巴赫·戈德堡变奏曲（Bach Goldberg Variations），馈入该神经网络，并根据所学的结构编写一首音乐样本。

### 注意

请注意，此示例归功于[《可视化和理解递归网络》](https://arxiv.org/abs/1506.02078)和[标题为“循环神经网络的不合理有效性”的文章](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)，该文章提供了许多想法和概念。

## 字符级别模型

如我们先前所见，Char RNN 模型可用于字符序列。 这类输入可以代表多种可能的语言。 以下是一些示例：

*   程式码
*   不同的人类语言（某些作者的写作风格的建模）
*   科学论文（tex）等

### 字符序列和概率表示

RNN 的输入内容需要一种清晰直接的表示方式。 因此，选择单热表示，可以方便地将其直接用于表征有限数量的可能结果（有限字符的数量是有限的并且以十为单位）的输出，并可以将其与 `Softmax`函数值。

因此，模型的输入是字符序列，模型的输出将是每个实例的数组序列。 数组的长度将与词汇表的大小相同，因此，给定先前输入的序列字符，每个数组位置将代表当前字符在此序列位置中的概率。

在下图中，我们观察到一个非常简化的设置模型，其中编码的输入单词和该模型预测单词`TEST`作为预期的输出：

![Character sequences and probability representation](img/00120.jpg)

### 将音乐编码为字符 -- ABC 音乐格式

搜索表示输入数据的格式时，如果可能的话，选择一种更简单但结构上均一的格式很重要。

关于音乐表示，ABC 格式是一种合适的选择，因为它的结构非常简单，使用的字符数有限，并且是 ASCII 字符集的子集。

#### ABC 格式数据组织

ABC 格式页面主要包含两个组件：标头和注释。

*   `Header`：标头包含一些键：值行，例如`X:[Reference number]`，`T:[Title]`，`M:[Meter]`，`K:[Key]`和`C[Composer]`。
*   注释：注释从 K 标题键之后开始，并列出每个小节的不同注释，以`|`字符分隔。

还有其他元素，但是通过以下示例，即使没有音乐训练，您也将了解格式的工作原理：

原始样本如下：

```py
X:1 
T:Notes 
M:C 
L:1/4 
K:C 
C, D, E, F,|G, A, B, C|D E F G|A B c d|e f g a|b c' d' e'|f' g' a' b'|] 

```

最终表示如下：

![ABC format data organization](img/00121.jpg)

巴赫·戈德堡的变化：

巴赫·戈德堡（Bach Goldberg）变奏曲是一组原始的咏叹调，并基于该咏叹调创作了 30 部作品，以巴赫的门徒约翰·哥特利布·戈德堡（Johann Gottlieb Goldberg）的名字命名，他可能是其主要的解释者。

在下一个清单和图中，我们将表示变体`Nr 1`的第一部分，因此您对我们将尝试模仿的文档结构有所了解：

```py
X:1  
T:Variation no. 1  
C:J.S.Bach  
M:3/4  
L:1/16  
Q:500  
V:2 bass  
K:G  
[V:1]GFG2- GDEF GAB^c |d^cd2- dABc defd |gfg2- gfed ^ceAG|  
[V:2]G,,2B,A, B,2G,2G,,2G,2 |F,,2F,E, F,2D,2F,,2D,2 |E,,2E,D, E,2G,2A,,2^C2|  
%  (More parts with V:1 and V:2) 

```

![ABC format data organization](img/00122.jpg)

### 有用的库和方法

在本节中，我们将学习在此示例中将使用的新函数。

### 保存和还原变量和模型

对于现实世界的应用来说，一项非常重要的能力是能够保存和检索整个模型。 TensorFlow 通过`tf.train.Saver`对象提供此功能。

该对象的主要方法如下：

*   `tf.train.Saver(args)`：这是构造函数。 这是主要参数的列表：
    *   `var_list`：这是一个列表，其中包含要保存的所有变量的列表。 例如，{`firstvar: var1`，`secondvar: var2`}。 如果不存在，请保存所有对象。
    *   `max_to_keep`：这表示要维护的最大检查点数。
    *   `write_version`：这是文件格式版本，实际上只有 1 个有效。
*   `tf.train.Saver.save`：此方法运行由构造函数添加的用于保存变量的 ops。 这需要当前会​​话，并且所有变量都已初始化。 主要参数如下：
    *   `session`：这是保存变量的会话
    *   `save_path`：这是检查点文件名的路径
    *   `global_step`：这是唯一的步骤标识符

此方法返回保存检查点的路径。

*   `tf.train.Saver.restore`：此方法恢复以前保存的变量。 主要参数如下：
    *   `session`：会话是要还原变量的位置
    *   `save_path`：这是先前由 save 方法，对 last_checkpoint（）的调用或提供的变量先前返回的变量

### 加载和保存的伪代码

在这里，我们将使用一些示例代码来构建用于保存和检索两个示例变量的最小结构。

#### 变量保存

以下是创建变量的代码：

```py
# Create some variables.simplevar = tf.Variable(..., name="simple")anothervar = tf.Variable(..., name="another")...# Add ops to save and restore all the variables.saver = tf.train.Saver()# Later, launch the model, initialize the variables, do some work, save the# variables to disk.with tf.Session() as sess:  sess.run(tf.initialize_all_variables())  # Do some work with the model.  ..  # Save the variables to disk.  save_path = saver.save(sess, "/tmp/model.ckpt")
```

#### 变量还原

以下是用于还原变量的代码：

```py
saver = tf.train.Saver()
# Later, launch the model, use the saver to restore variables from disk, and
# do some work with the model.
with tf.Session() as sess:
#Work with the restored model....

```

## 数据集说明和加载

对于此数据集，我们从 30 幅作品开始，然后生成其随机分布的`1000`个实例的列表：

```py
import random 
input = open('input.txt', 'r').read().split('X:') 
for i in range (1,1000): 
    print "X:" + input[random.randint(1,30)] + "\n_____________________________________\n" 

```

## 网络训练

网络训练的原始材料将是 ABC 格式的`30`作品。

### 注意

请注意，原始 ABC 文件位于[此链接](http://www.barfly.dial.pipex.com/Goldbergs.abc)。

然后，我们使用这个小程序（）。

对于此数据集，我们从`30`作品开始，然后生成一个随机分布的`1000`实例列表：

```py
import random 
input = open('original.txt', 'r').read().split('X:') 
for i in range (1,1000): 
    print "X:" + input[random.randint(1,30)] + "\n_____________________________________\n" 

```

然后我们执行以下命令来获取数据集：

```py
python generate_dataset.py > input.txt 

```

## 数据集预处理

生成的数据集在有用之前需要一些信息。 首先，它需要词汇的定义。

### 词汇定义

该过程的第一步是找到可以在原始文本中找到的所有不同字符，以便以后能够确定尺寸并填充单热编码输入。

在下图中，我们表示以 ABC 音乐格式找到的不同字符。 在这里，您可以看到标准中包含普通和特殊标点符号的内容：

![Vocabulary definition](img/00123.jpg)

### 建模架构

下面的行中描述了此 RNN 的模型，它是具有初始零状态的多层 LSTM：

```py
        cell_fn = rnn_cell.BasicLSTMCell  
        cell = cell_fn(args.rnn_size, state_is_tuple=True) 
        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers, state_is_tuple=True) 
        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length]) 
        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length]) 
        self.initial_state = cell.zero_state(args.batch_size, tf.float32) 
        with tf.variable_scope('rnnlm'): 
            softmax_w = tf.get_variable("softmax_w", [args.rnn_size, args.vocab_size])  
            softmax_b = tf.get_variable("softmax_b", [args.vocab_size])   
            with tf.device("/cpu:0"): 
                embedding = tf.get_variable("embedding", [args.vocab_size, args.rnn_size]) 
                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data)) 
                inputs = [tf.squeeze(input_, [1]) for input_ in inputs] 
        def loop(prev, _): 
            prev = tf.matmul(prev, softmax_w) + softmax_b 
            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1)) 
            return tf.nn.embedding_lookup(embedding, prev_symbol) 
        outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm') 
        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size]) 

```

## 损失函数说明

损失函数由 losss_by_example 函数定义。 这是基于一种称为“困惑性”的度量，该度量可测量概率分布预测样本的程度。 此度量在语言模型中广泛使用：

```py
        self.logits = tf.matmul(output, softmax_w) + softmax_b 
        self.probs = tf.nn.softmax(self.logits) 
        loss = seq2seq.sequence_loss_by_example([self.logits], 
                [tf.reshape(self.targets, [-1])], 
                [tf.ones([args.batch_size * args.seq_length])], 
                args.vocab_size) 
        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length 

```

## 停止条件

程序将迭代直到达到周期数和批号为止。 这是条件块：

```py
if (e==args.num_epochs-1 and b == data_loader.num_batches-1) 

```

## 结果描述

为了运行程序，首先使用以下代码运行训练脚本：

```py
python train.py 

```

然后，使用以下代码运行示例程序：

```py
python sample.py 

```

配置`X:1\n`的质数，这是一个可能的初始化字符序列，我们可以根据 RNN 的深度（建议 3）和长度（建议 512）获得几乎可以识别的完整构图。

根据现场诊断，获得了以下乐谱，将得到的字符序列复制到 [drawthedots.com](http://www.drawthedots.com/) 并进行简单的字符校正：

![Results description](img/00124.jpg)

## 完整源代码

以下是完整的源代码（`train.py`）：

```py
from __future__ import print_function 
import numpy as np 
import tensorflow as tf 

import argparse 
import time 
import os 
from six.moves import cPickle 
from utils import TextLoader 
from model import Model 
class arguments: 
    def __init__(self): 
        return 
def main(): 
    args = arguments()     
    train(args) 
def train(args): 
    args.data_dir='data/'; args.save_dir='save'; args.rnn_size =64; 
    args.num_layers=1;  args.batch_size=50;args.seq_length=50 
    args.num_epochs=5;args.save_every=1000; args.grad_clip=5\. 
    args.learning_rate=0.002; args.decay_rate=0.97 
    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length) 
    args.vocab_size = data_loader.vocab_size 
    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f: 
        cPickle.dump(args, f) 
    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f: 
        cPickle.dump((data_loader.chars, data_loader.vocab), f) 
    model = Model(args) 
    with tf.Session() as sess: 
        tf.initialize_all_variables().run() 
        saver = tf.train.Saver(tf.all_variables()) 
        for e in range(args.num_epochs): 
            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e))) 
            data_loader.reset_batch_pointer() 
            state = sess.run(model.initial_state) 
            for b in range(data_loader.num_batches): 
                start = time.time() 
                x, y = data_loader.next_batch() 
                feed = {model.input_data: x, model.targets: y} 
                for i, (c, h) in enumerate(model.initial_state): 
                    feed[c] = state[i].c 
                    feed[h] = state[i].h 
                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed) 
                end = time.time() 
                print("{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}" \ 
                    .format(e * data_loader.num_batches + b, 
                            args.num_epochs * data_loader.num_batches, 
                            e, train_loss, end - start)) 
                if (e==args.num_epochs-1 and b == data_loader.num_batches-1): # save for the last result 
                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt') 
                    saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b) 
                    print("model saved to {}".format(checkpoint_path)) 

if __name__ == '__main__': 
    main() 

```

以下是完整的源代码（`model.py`）：

```py
import tensorflow as tf
from tensorflow.python.ops import rnn_cell
from tensorflow.python.ops import seq2seq
import numpy as np

class Model():
    def __init__(self, args, infer=False):
        self.args = args
        if infer: #When we sample, the batch and sequence lenght are = 1
            args.batch_size = 1
            args.seq_length = 1
        cell_fn = rnn_cell.BasicLSTMCell #Define the internal cell structure
        cell = cell_fn(args.rnn_size, state_is_tuple=True)
        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers, state_is_tuple=True)
        #Build the inputs and outputs placeholders, and start with a zero internal values
        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])
        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])
        self.initial_state = cell.zero_state(args.batch_size, tf.float32)
        with tf.variable_scope('rnnlm'):
            softmax_w = tf.get_variable("softmax_w", [args.rnn_size, args.vocab_size]) #Final w
            softmax_b = tf.get_variable("softmax_b", [args.vocab_size]) #Final bias
            with tf.device("/cpu:0"):
                embedding = tf.get_variable("embedding", [args.vocab_size, args.rnn_size])
                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))
                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
        def loop(prev, _):
            prev = tf.matmul(prev, softmax_w) + softmax_b
            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))
            return tf.nn.embedding_lookup(embedding, prev_symbol)
        outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')
        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])
        self.logits = tf.matmul(output, softmax_w) + softmax_b
        self.probs = tf.nn.softmax(self.logits)
        loss = seq2seq.sequence_loss_by_example([self.logits],
            [tf.reshape(self.targets, [-1])],
            [tf.ones([args.batch_size * args.seq_length])],
            args.vocab_size)
        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length
        self.final_state = last_state
        self.lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost,        tvars),
        args.grad_clip)
        optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))
    def sample(self, sess, chars, vocab, num=200, prime='START', sampling_type=1):
        state = sess.run(self.cell.zero_state(1, tf.float32))
        for char in prime[:-1]:
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [state] = sess.run([self.final_state], feed)
        def weighted_pick(weights):
            t = np.cumsum(weights)
            s = np.sum(weights)
            return(int(np.searchsorted(t, np.random.rand(1)*s)))
        ret = prime
        char = prime[-1]
        for n in range(num):
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [probs, state] = sess.run([self.probs, self.final_state], feed)
            p = probs[0]
            sample = weighted_pick(p)
            pred = chars[sample]
            ret += pred
            char = pred
        return ret
```

以下是完整的源代码（`sample.py`）：

```py
from __future__ import print_function

import numpy as np
import tensorflow as tf
import time
import os
from six.moves import cPickle
from utils import TextLoader
from model import Model
from six import text_type

class arguments: #Generate the arguments class
    save_dir= 'save'
    n=1000
    prime='x:1\n'
    sample=1 

def main():
    args = arguments()
    sample(args)   #Pass the argument object

def sample(args):
    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:
        saved_args = cPickle.load(f) #Load the config from the standard file
    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:

        chars, vocab = cPickle.load(f) #Load the vocabulary
    model = Model(saved_args, True) #Rebuild the model
    with tf.Session() as sess:
        tf.initialize_all_variables().run() 
        saver = tf.train.Saver(tf.all_variables())   
        ckpt = tf.train.get_checkpoint_state(args.save_dir) #Retrieve the chkpoint
        if ckpt and ckpt.model_checkpoint_path:
            saver.restore(sess, ckpt.model_checkpoint_path) #Restore the model
            print(model.sample(sess, chars, vocab, args.n, args.prime, args.sample))
            #Execute the model, generating a n char sequence
            #starting with the prime sequence
if __name__ == '__main__':
    main()

```

以下是完整的源代码（`utils.py`）：

```py
import codecs
import os
import collections
from six.moves import cPickle
import numpy as np

class TextLoader():
    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.encoding = encoding

        input_file = os.path.join(data_dir, "input.txt")
        vocab_file = os.path.join(data_dir, "vocab.pkl")
        tensor_file = os.path.join(data_dir, "data.npy")

        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):
            print("reading text file")
            self.preprocess(input_file, vocab_file, tensor_file)
        else:
            print("loading preprocessed files")
            self.load_preprocessed(vocab_file, tensor_file)
        self.create_batches()
        self.reset_batch_pointer()

    def preprocess(self, input_file, vocab_file, tensor_file):
        with codecs.open(input_file, "r", encoding=self.encoding) as f:
            data = f.read()
        counter = collections.Counter(data)
        count_pairs = sorted(counter.items(), key=lambda x: -x[1])
        self.chars, _ = zip(*count_pairs)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        with open(vocab_file, 'wb') as f:
            cPickle.dump(self.chars, f)
        self.tensor = np.array(list(map(self.vocab.get, data)))
        np.save(tensor_file, self.tensor)

    def load_preprocessed(self, vocab_file, tensor_file):
        with open(vocab_file, 'rb') as f:
            self.chars = cPickle.load(f)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        self.tensor = np.load(tensor_file)
        self.num_batches = int(self.tensor.size / (self.batch_size *
                                                   self.seq_length))

    def create_batches(self):
        self.num_batches = int(self.tensor.size / (self.batch_size *
                                                   self.seq_length))

        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]
        xdata = self.tensor
        ydata = np.copy(self.tensor)
        ydata[:-1] = xdata[1:]
        ydata[-1] = xdata[0]
        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)
        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)

    def next_batch(self):
        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]
        self.pointer += 1
        return x, y

    def reset_batch_pointer(self):
        self.pointer = 0

```