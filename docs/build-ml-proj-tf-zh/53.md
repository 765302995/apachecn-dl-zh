# 第 9 章 大规模运行模型 -- GPU 和服务

到目前为止，我们一直在运行在主机的主 CPU 上运行的代码。 这意味着最多使用所有不同的处理器内核（低端处理器使用 2 或 4 个内核，高级处理器使用多达 16 个内核）。

在过去的十年中，通用处理单元（GPU）已成为所有高表现计算设置中无处不在的部分。 它的大量固有并行度非常适合于高维矩阵乘法以及机器学习模型训练和运行所需的其他运算。

尽管如此，即使拥有真正强大的计算节点，也存在许多任务，即使是最强大的单个服务器也无法应对。

因此，必须开发一种训练和运行模型的分布式方法。 这是分布式 TensorFlow 的原始功能。

在本章中，您将：

*   了解如何发现 TensorFlow 可用的计算资源
*   了解如何将任务分配给计算节点中的任何不同计算单元
*   了解如何记录 GPU 操作
*   了解如何不仅在主主机中而且在许多分布式单元的集群中分布计算